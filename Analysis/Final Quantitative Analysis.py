# -*- coding: utf-8 -*-
"""Copy of Final Quantitative Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T6NYUxe0d4R_O45w2L3bnaShzHEDB6Dz
"""

from glob import glob
from json import loads,dumps

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import time
from datetime import datetime
import warnings

# Filter out depreciation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from google.colab import drive

drive.mount('/content/drive/')

"""## Making CSVs
Function to parse our raw txt files and create one csv file which contains the information of each video that was watched and the advertisements that were played on them
"""

def makeVideoCSV(country):

  path = "drive/My Drive/Topics in IR Project/Data Collection/Labelled/"+country+"/Top 75/*/*"
  pathForCSV = "drive/My Drive/Topics in IR Project/data_csvs/17May/"+country+".csv"

  all_videos = glob(path)
  print(all_videos)

  print("Total videos:",len(all_videos))

  ad_dict = {
      "main video url":[],
      "main video duration": [],
      "video ad ids": [],
      "ad skippable":[],
      "ad counts": [],
      "total unique ads": [],
      "banner ad img srcs": [],
      "banner ad urls": [],
      "sidebar":[],
      "in-feed":[]

  }

  for vid in all_videos:

      f = open(vid,'r')
      data = loads(f.read())
      # print(data)




      banner_imgs = []
      banner_urls = []

      video_ad_ids = []
      video_ad_counts = []
      ad_skippable = []

      sidebar_ads = []

      infeed_ads = []

      for key,value in data.items():
        if (key=="Main_Video"):
          ad_dict["main video url"].append(data[key]['Url'])
          ad_dict["main video duration"].append(data[key]['Total Duration'])
          ad_dict["total unique ads"].append(data[key]['UniqueAds'])
        elif (key.startswith("Banner Ad")):
          banner_urls.append(data[key]['Url'])
          banner_imgs.append(data[key]['ImgSrc'])
        elif (key=="Sidebar"):
          sidebar_ads.append(data[key])
        elif (key=="In-feed"):
          infeed_ads.append(data[key])
        else:
            video_ad_ids.append(key)
            video_ad_counts.append(data[key]['Count'])
            ad_skippable.append(data[key]['Skippable'])




      ad_dict['sidebar'].append(sidebar_ads)

      ad_dict["in-feed"].append(infeed_ads)

      ad_dict['banner ad img srcs'].append(banner_imgs)
      ad_dict['banner ad urls'].append(banner_urls)



      ad_dict['video ad ids'].append(video_ad_ids)
      ad_dict['ad counts'].append(video_ad_counts)
      ad_dict['ad skippable'].append(ad_skippable)

  df = pd.DataFrame.from_dict(ad_dict)
  display(df)

  df.to_csv(pathForCSV)

def makeULVideoCSV(): ##only path changed for our unlabelled video dataset (same function as above)

  path = "drive/My Drive/Topics in IR Project/Data Collection/Unlabelled/*/*"
  pathForCSV = "drive/My Drive/data_csvs/unlabelled.csv"

  all_videos = glob(path)
  print(all_videos)

  print("Total videos:",len(all_videos))

  ad_dict = {
      "main video url":[],
      "main video duration": [],
      "video ad ids": [],
      "ad skippable":[],
      "ad counts": [],
      "total unique ads": [],
      "banner ad img srcs": [],
      "banner ad urls": [],
      "sidebar":[],
      "in-feed":[]

  }

  for vid in all_videos:

      f = open(vid,'r')
      data = loads(f.read())
      # print(data)




      banner_imgs = []
      banner_urls = []

      video_ad_ids = []
      video_ad_counts = []
      ad_skippable = []

      sidebar_ads = []

      infeed_ads = []

      for key,value in data.items():
        if (key=="Main_Video"):
          ad_dict["main video url"].append(data[key]['Url'])
          ad_dict["main video duration"].append(data[key]['Total Duration'])
          ad_dict["total unique ads"].append(data[key]['UniqueAds'])
        elif (key.startswith("Banner Ad")):
          banner_urls.append(data[key]['Url'])
          banner_imgs.append(data[key]['ImgSrc'])
        elif (key=="Sidebar"):
          sidebar_ads.append(data[key])
        elif (key=="In-feed"):
          infeed_ads.append(data[key])
        else:
            video_ad_ids.append(key)
            video_ad_counts.append(data[key]['Count'])
            ad_skippable.append(data[key]['Skippable'])




      ad_dict['sidebar'].append(sidebar_ads)

      ad_dict["in-feed"].append(infeed_ads)

      ad_dict['banner ad img srcs'].append(banner_imgs)
      ad_dict['banner ad urls'].append(banner_urls)



      ad_dict['video ad ids'].append(video_ad_ids)
      ad_dict['ad counts'].append(video_ad_counts)
      ad_dict['ad skippable'].append(ad_skippable)

  df = pd.DataFrame.from_dict(ad_dict)
  display(df)

  df.to_csv(pathForCSV)

"""## Extracting all the unique ads that played in each country








"""

def uniqueAdsCSV(country):

  path = "drive/My Drive/Topics in IR Project/Data Collection/Labelled/"+country+"/Top 75/*/*"

  all_videos = glob(path)


  unique_banner = {"Url":[],"ImgSrc":[]}
  unique_video = []
  unique_sidebar = {"Img":[],"Title":[],"Description":[],"Link":[]}
  unique_infeed = {"Img":[],"Title":[],"Link":[],"Channel":[]}


  for vid in all_videos:

    f = open(vid,'r')
    data = loads(f.read())
    # print(data)

    for key,value in data.items():
      if (key=="Main_Video"):
        pass

      elif (key.startswith("Banner Ad")):

        if (data[key]["ImgSrc"] not in unique_banner["ImgSrc"]):
          unique_banner["Url"].append(data[key]["Url"])
          unique_banner["ImgSrc"].append(data[key]["ImgSrc"])

      elif (key=="Sidebar"):
        if ((data[key]["Img"] not in unique_sidebar["Img"]) or (data[key]["Link"] not in unique_sidebar["Link"])):
          unique_sidebar["Img"].append(data[key]["Img"])
          unique_sidebar["Title"].append(data[key]["Title"])
          unique_sidebar["Description"].append(data[key]["Description"])
          unique_sidebar["Link"].append(data[key]["Link"])

      elif (key=="In-feed"):
        if (data[key]["Link"] not in unique_infeed["Link"]):
          unique_infeed["Img"].append(data[key]["Img"])
          unique_infeed["Title"].append(data[key]["Title"])
          unique_infeed["Link"].append(data[key]["Link"])
          unique_infeed["Channel"].append(data[key]["Channel"])

      else:
        if key not in unique_video:
          unique_video.append(key)

  print("Total unique video ads:",len(unique_video))
  print("Total unique banner ads:",len(unique_banner))
  print("Total unique sidebar ads:",len(unique_sidebar))
  print("Total unique in-feed ads:",len(unique_infeed))


  unique_video = [x.strip() for x in unique_video]

  unique_video_df = pd.DataFrame.from_dict({"ad_video_id":unique_video})
  unique_banner_df = pd.DataFrame.from_dict(unique_banner)
  unique_sidebar = pd.DataFrame.from_dict(unique_sidebar)
  unique_infeed = pd.DataFrame.from_dict(unique_infeed)

  unique_video_df.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_"+country+".csv")
  unique_banner_df.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_"+country+".csv")
  unique_sidebar.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_"+country+".csv")
  unique_infeed.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_"+country+".csv")

"""### Running code for all regions and saving csv for further processing"""

makeVideoCSV('UK')
uniqueAdsCSV('UK')

makeVideoCSV('Pakistan')
uniqueAdsCSV('Pakistan')

makeVideoCSV('Germany')
uniqueAdsCSV('Germany')

makeVideoCSV('US')
uniqueAdsCSV('US')

makeVideoCSV('Bangladesh')
uniqueAdsCSV('Bangladesh')

makeVideoCSV('Sri Lanka')
uniqueAdsCSV('Sri Lanka')

makeVideoCSV('Sweden')
uniqueAdsCSV('Sweden')

makeVideoCSV('Germany')
uniqueAdsCSV('Germany')

makeVideoCSV('Venezuela')
uniqueAdsCSV('Venezuela')

makeVideoCSV('Morocco')
uniqueAdsCSV('Morocco')

us = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_US.csv")

sweden = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sweden.csv")

pak = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Pakistan.csv")

uk = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_UK.csv")
france = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_France.csv")

bangladesh = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Bangladesh.csv")
srilanka = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sri Lanka.csv")

"""### Randomly selecting a sample of ads to watch for each country
Should be noted that for some countries we only needed to choose 100 ads because 50 had already been tagged as a part of prior work
"""

pak_tagged = pd.read_csv("drive/My Drive/pak_tagged.csv", header=None)

bangladesh_tagged = pd.read_csv("drive/My Drive/bangladesh_tagged.csv",header=None)

srilanka_tagged = pd.read_csv("drive/My Drive/srilanka_tagged.csv",header=None)

unlabelled_tagged = pd.read_csv("drive/My Drive/unlabelled_tagged.csv",header=None)

uk_tagged = pd.read_csv("drive/My Drive/uk_tagged.csv",header=None)
uk_new = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/uk_tag.csv")
uk_tagged = uk_tagged.rename(columns={0: 'ad_video_id'})
uk_l = uk_tagged.merge(uk_new, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)

ger = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Germany.csv")
ger_l = ger.sample(n=150, random_state=28)

ger_l.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/germany_tag.csv")

france_tagged = pd.read_csv("drive/My Drive/france_tagged.csv",header=None)
france_new = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/france_tag.csv")
france_tagged = france_tagged.rename(columns={0: 'ad_video_id'})
france_l = france_tagged.merge(france_new, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)

france_filtered = france.merge(uk_l, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)
random_france = france_filtered.sample(n=26, random_state=57)

uk_filtered = uk.merge(uk_l, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)
random_uk = uk_filtered.sample(n=5, random_state=57)

bangladesh_tagged = bangladesh_tagged.rename(columns={0: 'ad_video_id'})
srilanka_tagged = srilanka_tagged.rename(columns={0: 'ad_video_id'})
pak_tagged = pak_tagged.rename(columns={0: 'ad_video_id'})

random_sweden = sweden.sample(n=150, random_state=23)
random_sweden.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sweden_tag.csv")

us_filtered = us.merge(us_tagged, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)
uk_filtered = uk.merge(uk_tagged, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)
france_filtered = france.merge(france_tagged, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)

bd_filtered = bangladesh.merge(bangladesh_tagged, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)
srilanka_filtered = srilanka.merge(srilanka_tagged, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)
pak_filtered = pak.merge(pak_tagged, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both'].drop('_merge', axis=1)

random_us = us_filtered.sample(n=100, random_state=42)
random_us.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/us_tag.csv")

random_uk = uk_filtered.sample(n=100, random_state=57)
random_uk.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/uk_tag.csv")

random_fr = france_filtered.sample(n=100, random_state=31)
random_fr.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/france_tag.csv")

random_bd = bd_filtered.sample(n=100, random_state=18)
random_bd.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/bangladesh_tag.csv")

random_srilanka = srilanka_filtered.sample(n=100, random_state=79)
random_srilanka.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/srilanka_tag.csv")

random_pak = pak_filtered.sample(n=100, random_state=25)
random_pak.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/pak_tag.csv")

"""## Querying ads using Youtube API"""

import time
import isodate #needed to convert time returned by Youtube API in to seconds

pip install isodate

KEY = "AIzaSyA1NDPPW-hw0f3F8OJf-U5nEXsiXlcQKrg" # Enter YouTube API key here to make requests

from unicodedata import category
from googleapiclient.discovery import build

def GetAdDetails(api_key,ad_ID):
    # This function gets the ad details from the Youtube API given the video id of the advertisement
    # Arguments:
    #           api_key (string): Your api key to be used to make requests
    #           ad_id (string): The video id of the advertisement

    youtube = build('youtube','v3',developerKey=api_key)

    request = youtube.videos().list(
        part="snippet,status,contentDetails,topicDetails",
        id = ad_ID
    )
    response = request.execute()

    video_title = "N/I"
    video_desc = "N/I"
    channel_id = "N/I"
    channel_title = "N/I"
    channel_title = "N/I"
    thumbnail = {"url":["N/I"]}
    tags = "N/I"
    category_id = "N/I"
    madeForKids = "N/I"
    content_rating = "N/I"
    duration = "N/I"


    if (response["pageInfo"]["totalResults"]>0):

      video_title = response['items'][0]['snippet']['title']
      video_desc = response['items'][0]['snippet']['description']
      channel_id = response['items'][0]['snippet']['channelId']
      channel_title = response['items'][0]['snippet']['channelTitle']
      thumbnail = response['items'][0]['snippet']['thumbnails']['default']

      try:
          tags = response['items'][0]['snippet']['tags']
      except:
          tags = "N/A"

      category_id = response['items'][0]['snippet']['categoryId']
      madeForKids = response['items'][0]['status']['madeForKids']


      content_rating = response['items'][0]['contentDetails']['contentRating']

      duration = response['items'][0]['contentDetails']['duration']

    else:
      print("Something went wrong:",ad_ID)


    ad_info = {}

    ad_info["title"]=video_title
    ad_info["description"]=video_desc
    ad_info["channelId"]=channel_id
    ad_info["channelTitle"]=channel_title
    ad_info["thumbnail"] = thumbnail
    ad_info["tags"]= tags
    ad_info["categoryId"]=category_id
    ad_info["madeForKids"]=madeForKids
    ad_info["contentRating"]=content_rating
    ad_info["duration"]=duration

    return(dumps(ad_info))

def GetUniqueAds(country):
  path = "drive/My Drive/Topics in IR Project/Data Collection/Labelled/"+country+"/Top 75/*/*"

  all_videos = glob(path)



  unique_video = []


  for vid in all_videos:

    f = open(vid,'r')
    data = loads(f.read())

    for key,value in data.items():
      key = key.strip()
      if (key=="Main_Video" or key.startswith("Banner Ad") or key=="Sidebar" or key=="In-feed"):
        pass

      else:
        if key not in unique_video:
          unique_video.append(key)

  print("Total unique video ads:",len(unique_video))



  unique_video = [x.strip() for x in unique_video]

  return unique_video

def GetULUniqueAds():
  path = "drive/My Drive/Topics in IR Project/Data Collection/Unlabelled"

  all_videos = glob(path)



  unique_video = []


  for vid in all_videos:

    f = open(vid,'r')
    data = loads(f.read())

    for key,value in data.items():
      key = key.strip()
      if (key=="Main_Video" or key.startswith("Banner Ad") or key=="Sidebar" or key=="In-feed"):
        pass

      else:
        if key not in unique_video:
          unique_video.append(key)

  print("Total unique video ads:",len(unique_video))



  unique_video = [x.strip() for x in unique_video]

  return unique_video

def AdDetailsCSV(country):

  unique_video = GetUniqueAds(country)

  video_ad_details = {
      "id":[],
      "title":[],
      "description":[],
      "channelId":[],
      "channelTitle":[],
      "thumbnail":[],
      "tags":[],
      "categoryId":[],
      "madeForKids":[],
      "duration":[]
  }

  for vid in unique_video:
    res = loads(GetAdDetails(KEY,vid))
    # print(res["title"])

    video_ad_details["id"].append(vid)
    video_ad_details["title"].append(res["title"])
    video_ad_details["description"].append(res["description"])
    video_ad_details["channelId"].append(res["channelId"])
    video_ad_details["channelTitle"].append(res["channelTitle"])
    video_ad_details["thumbnail"].append(res["thumbnail"]["url"])
    video_ad_details["tags"].append(res["tags"])
    video_ad_details["categoryId"].append(res["categoryId"])
    video_ad_details["madeForKids"].append(res["madeForKids"])
    video_ad_details["duration"].append(res["duration"])
    time.sleep(0.5)

  ## getting times of the ads in a readable format

  duration_seconds = []

  for i in range(0,len(video_ad_details["duration"])):
    if (video_ad_details["duration"][i] == "N/I"):
      duration_seconds.append("")
      continue
    else:
      dur = video_ad_details["duration"][i]
      in_seconds = isodate.parse_duration(dur)

      final_time = in_seconds.total_seconds()
      duration_seconds.append(final_time)

  video_ad_details["duration seconds"]= duration_seconds
  return video_ad_details
  # ad_df = pd.DataFrame.from_dict("drive/My Drive/Topics in IR Project/csvs/Top 35/ad_details_"+country+".csv")

def ULAdDetailsCSV():

  unique_video = GetULUniqueAds()

  video_ad_details = {
      "id":[],
      "title":[],
      "description":[],
      "channelId":[],
      "channelTitle":[],
      "thumbnail":[],
      "tags":[],
      "categoryId":[],
      "madeForKids":[],
      "duration":[]
  }

  for vid in unique_video:
    res = loads(GetAdDetails(KEY,vid))
    # print(res["title"])

    video_ad_details["id"].append(vid)
    video_ad_details["title"].append(res["title"])
    video_ad_details["description"].append(res["description"])
    video_ad_details["channelId"].append(res["channelId"])
    video_ad_details["channelTitle"].append(res["channelTitle"])
    video_ad_details["thumbnail"].append(res["thumbnail"]["url"])
    video_ad_details["tags"].append(res["tags"])
    video_ad_details["categoryId"].append(res["categoryId"])
    video_ad_details["madeForKids"].append(res["madeForKids"])
    video_ad_details["duration"].append(res["duration"])
    time.sleep(0.5)

  ## getting times of the ads in a readable format

  duration_seconds = []

  for i in range(0,len(video_ad_details["duration"])):
    if (video_ad_details["duration"][i] == "N/I"):
      duration_seconds.append("")
      continue
    else:
      dur = video_ad_details["duration"][i]
      in_seconds = isodate.parse_duration(dur)

      final_time = in_seconds.total_seconds()
      duration_seconds.append(final_time)

  video_ad_details["duration seconds"]= duration_seconds
  return video_ad_details
  # ad_df = pd.DataFrame.from_dict("drive/My Drive/Topics in IR Project/csvs/Top 35/ad_details_"+country+".csv")

def AdDetailsCSV(country):

  unique_video = GetUniqueAds(country)

  video_ad_details = {
      "id":[],
      "title":[],
      "description":[],
      "channelId":[],
      "channelTitle":[],
      "thumbnail":[],
      "tags":[],
      "categoryId":[],
      "madeForKids":[],
      "duration":[]
  }

  for vid in unique_video:
    res = loads(GetAdDetails(KEY,vid))
    # print(res["title"])

    video_ad_details["id"].append(vid)
    video_ad_details["title"].append(res["title"])
    video_ad_details["description"].append(res["description"])
    video_ad_details["channelId"].append(res["channelId"])
    video_ad_details["channelTitle"].append(res["channelTitle"])
    video_ad_details["thumbnail"].append(res["thumbnail"]["url"])
    video_ad_details["tags"].append(res["tags"])
    video_ad_details["categoryId"].append(res["categoryId"])
    video_ad_details["madeForKids"].append(res["madeForKids"])
    video_ad_details["duration"].append(res["duration"])
    time.sleep(0.5)

  ## getting times of the ads in a readable format

  duration_seconds = []

  for i in range(0,len(video_ad_details["duration"])):
    if (video_ad_details["duration"][i] == "N/I"):
      duration_seconds.append("")
      continue
    else:
      dur = video_ad_details["duration"][i]
      in_seconds = isodate.parse_duration(dur)

      final_time = in_seconds.total_seconds()
      duration_seconds.append(final_time)

  video_ad_details["duration seconds"]= duration_seconds
  return video_ad_details
  # ad_df = pd.DataFrame.from_dict("drive/My Drive/Topics in IR Project/csvs/Top 35/ad_details_"+country+".csv")

"""###Running code to get ad details for all the ads in every country"""

df = AdDetailsCSV("UK")

uk_ad_details_df = pd.DataFrame.from_dict(df)

uk_ad_details_df.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_UK.csv")

df = AdDetailsCSV("US")
us_ad_details_df = pd.DataFrame.from_dict(df)
us_ad_details_df.to_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_US.csv")

df = AdDetailsCSV("Venezuela")
uk_ad_details_df = pd.DataFrame.from_dict(df)
uk_ad_details_df.to_csv("drive/My Drive/data_csvs/17May/ad_details_Venezuela.csv")

df = AdDetailsCSV("France")
fr_ad_details_df = pd.DataFrame.from_dict(df)
fr_ad_details_df.to_csv("drive/My Drive/data_csvs/ad_details_France.csv")

makeVideoCSV('Pakistan')
uniqueAdsCSV('Pakistan')

dictionary_pak = AdDetailsCSV("Pakistan")
pak_ad_details_df = pd.DataFrame.from_dict(dictionary_pak)
pak_ad_details_df.to_csv("drive/My Drive/data_csvs/ad_details_Pakistan.csv")

makeULVideoCSV()
uniqueAdsCSV()

makeVideoCSV('US')
uniqueAdsCSV('US')

dictionary_us = AdDetailsCSV("US")
us_ad_details_df = pd.DataFrame.from_dict(dictionary_us)
us_ad_details_df.to_csv("drive/My Drive/data_csvs/ad_details_US.csv")

makeVideoCSV('Sri Lanka')
uniqueAdsCSV('Sri Lanka')

dictionary_sri = AdDetailsCSV("Sri Lanka")
sri_ad_details_df = pd.DataFrame.from_dict(dictionary_sri)
sri_ad_details_df.to_csv("drive/My Drive/data_csvs/ad_details_SriLanka.csv")

makeVideoCSV('Bangladesh')
uniqueAdsCSV('Bangladesh')

dictionary_ban = AdDetailsCSV("Bangladesh")
ban_ad_details_df = pd.DataFrame.from_dict(dictionary_ban)
ban_ad_details_df.to_csv("drive/My Drive/data_csvs/ad_details_Bangladesh.csv")

"""# Quantitative Analysis + Visualization"""

from glob import glob
from json import loads,dumps

import matplotlib.pyplot as plt
import pandas as pd
import time
from datetime import datetime

import ast

import seaborn as sns
import numpy as np
import scipy.stats as stats

"""###Functions"""

def addTime(df, ad_df): ## function to calculate total advertisement time
  total_time = []
  for ind in df.index:
    ads = ast.literal_eval(df["video ad ids"][ind])
    freqs = ast.literal_eval(df["ad counts"][ind])
    sum = 0
    for i in range(0,len(ads)):
        ad = ads[i].strip()
        freq = freqs[i]
        secs = ad_df[ad_df['id']==ad]['duration seconds'].iloc[0]
        sum+=secs*freq
    total_time.append(sum)
  return total_time

def addCount(df, ad_df): # function to calculate total number of advertisements
  total_ads = []
  for ind in df.index:
    ads = ast.literal_eval(df["video ad ids"][ind])
    freqs = ast.literal_eval(df["ad counts"][ind])
    sum = 0
    for i in range(0,len(ads)):
      freq = freqs[i]
      sum+=freq
    total_ads.append(sum)
  return total_ads

def skippable(df, ad_df):
  skip_ads = []
  for ind in df.index:
    ads = ast.literal_eval(df["video ad ids"][ind])
    freqs = ast.literal_eval(df["ad counts"][ind])
    skips = ast.literal_eval(df["ad skippable"][ind])

    sum = 0
    for i in range(0,len(ads)):
      freq = freqs[i]
      skip = skips[i]
      if skip:
        sum+=freq
    skip_ads.append(sum)
  return skip_ads

def getWatchTime(df):
  total_time=[]
  for index,row in df.iterrows():
    total_time.append(row["main video duration"] + row["total ad time"])
  return total_time

def skippableTime(df, ad_df):
  skip_ads = []
  for ind in df.index:
    ads = ast.literal_eval(df["video ad ids"][ind])
    freqs = ast.literal_eval(df["ad counts"][ind])
    skips = ast.literal_eval(df["ad skippable"][ind])


    sum = 0
    for i in range(0,len(ads)):
      ad = ads[i].strip()
      freq = freqs[i]
      skip = skips[i]
      # print(skip)
      if skip:
        sum+=freq*5
      else:
        secs = ad_df[ad_df['id']==ad]['duration seconds'].iloc[0]
        sum+=freq*secs
    skip_ads.append(sum)
  return skip_ads

def getTimeDF(df):
  times_df = pd.DataFrame()
  times_df["video time"] = df["main video duration"].copy()
  times_df["ad time"] = df["total ad time"].copy()
  times_df["skip time"] = df["skip ad time"].copy()
  times_df["total time"] = getWatchTime(df)
  times_df["video time"] = df["main video duration"].copy()
  times_df = times_df.sort_values("total time")
  return times_df

def getSkippableDurations(df,ad_df):
    skip_ads = pd.DataFrame(columns=["ad", "duration"])
    for ind in df.index:
        ads = ast.literal_eval(df["video ad ids"][ind])
        skips = ast.literal_eval(df["ad skippable"][ind])
        for i in range(len(ads)):
            ad = ads[i].strip()
            skip = skips[i]
            if skip == 1 and ad not in skip_ads["ad"].values:
                duration = ad_df.loc[ad_df['id'] == ad, 'duration seconds'].iloc[0]
                skip_ads = skip_ads.append({"ad": ad, "duration": duration}, ignore_index=True)
    return skip_ads

def getUnSkippableDurations(df, ad_df):
    unskip_ads = pd.DataFrame(columns=["ad", "duration"])
    for ind in df.index:
        ads = ast.literal_eval(df["video ad ids"][ind])
        skips = ast.literal_eval(df["ad skippable"][ind])
        for i in range(len(ads)):
            ad = ads[i].strip()
            skip = skips[i]
            if skip == 0 and ad not in unskip_ads["ad"].values:
                duration = ad_df.loc[ad_df['id'] == ad, 'duration seconds'].iloc[0]
                unskip_ads = unskip_ads.append({"ad": ad, "duration": duration}, ignore_index=True)
    return unskip_ads

def histobar_maker(pol_1_dist, pol_2_dist):
  values_pol_1 = [0 for _ in range(7)]
  values_pol_2 = [0 for _ in range(7)]
  five_plus_pol_1 = 0
  five_plus_pol_2 = 0

  for i in range(len(pol_1_dist)):
    if pol_1_dist['num of ads'][i].item() == 0:
      values_pol_1[0] += 1
    elif pol_1_dist['num of ads'][i].item() == 1:
      values_pol_1[1] += 1
    elif pol_1_dist['num of ads'][i].item() == 2:
      values_pol_1[2] += 1
    elif pol_1_dist['num of ads'][i].item() == 3:
      values_pol_1[3] += 1
    elif pol_1_dist['num of ads'][i].item() == 4:
      values_pol_1[4] += 1
    elif pol_1_dist['num of ads'][i].item() == 5:
      values_pol_1[5] += 1
      five_plus_pol_1 += 1
    elif pol_1_dist['num of ads'][i].item() == 6:
      values_pol_1[6] += 1
      five_plus_pol_1 += 1
    elif pol_1_dist['num of ads'][i].item() > 6:
      five_plus_pol_1 += 1

  for i in range(len(pol_2_dist)):
    if pol_2_dist['num of ads'][i].item() == 0:
      values_pol_2[0] += 1
    elif pol_2_dist['num of ads'][i].item() == 1:
      values_pol_2[1] += 1
    elif pol_2_dist['num of ads'][i].item() == 2:
      values_pol_2[2] += 1
    elif pol_2_dist['num of ads'][i].item() == 3:
      values_pol_2[3] += 1
    elif pol_2_dist['num of ads'][i].item() == 4:
      values_pol_2[4] += 1
    elif pol_2_dist['num of ads'][i].item() == 5:
      values_pol_2[5] += 1
      five_plus_pol_2 += 1
    elif pol_2_dist['num of ads'][i].item() == 6:
      values_pol_2[6] += 1
      five_plus_pol_2 += 1
    elif pol_2_dist['num of ads'][i].item() > 6:
      five_plus_pol_2 += 1

  values_pol_1 = np.array(values_pol_1)
  values_pol_2 = np.array(values_pol_2)

  values_pol_1 = (values_pol_1 / len(pol_1_dist)) * 100
  values_pol_2 = (values_pol_2 / len(pol_2_dist)) * 100

  five_plus_pol_1 = (five_plus_pol_1 / len(pol_1_dist)) * 100
  five_plus_pol_2 = (five_plus_pol_2 / len(pol_2_dist)) * 100

  return values_pol_1, values_pol_2, five_plus_pol_1, five_plus_pol_2

def getDist(df, country):
  ad_num_dist = pd.DataFrame()
  ad_num_dist["num of ads"] = df["total ad count"].copy()
  # print(len(ad_num_dist['num of ads']))
  ad_num_dist["ad type"] = ['video' for i in range(len(ad_num_dist['num of ads']))]
  ad_num_dist["country"] = [country for i in range(len(ad_num_dist["num of ads"]))]
  return ad_num_dist

def get_counts_of_ad_types():
  vid_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_US.csv")
  vid_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_UK.csv")
  vid_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Germany.csv")
  vid_swe_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sweden.csv")
  vid_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_France.csv")
  vid_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Pakistan.csv")
  vid_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Bangladesh.csv")
  vid_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sri Lanka.csv")
  vid_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Morocco.csv")
  vid_ven_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Venezuela.csv")

  vid_us_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_US_unlabelled.csv")
  vid_uk_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_UK_unlabelled.csv")
  vid_ger_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Germany_unlabelled.csv")
  vid_swe_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sweden_unlabelled.csv")
  vid_fr_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_France_unlabelled.csv")
  vid_pak_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Pak_unlabelled.csv")
  vid_ban_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Bangladesh_unlabelled.csv")
  vid_sri_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_SriLanka_unlabelled.csv")
  vid_mor_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Morocco_unlabelled.csv")
  vid_ven_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Venezuela_unlabelled.csv")

  banner_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_US.csv")
  banner_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_UK.csv")
  banner_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Germany.csv")
  banner_swe_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Sweden.csv")
  banner_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_France.csv")
  banner_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Pakistan.csv")
  banner_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Bangladesh.csv")
  banner_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Sri Lanka.csv")
  banner_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Morocco.csv")
  banner_ven_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Venezuela.csv")

  banner_us_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_US_unlabelled.csv")
  banner_uk_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_UK_unlabelled.csv")
  banner_ger_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Germany_unlabelled.csv")
  banner_swe_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Sweden_unlabelled.csv")
  banner_fr_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_France_unlabelled.csv")
  banner_pak_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Pak_unlabelled.csv")
  banner_ban_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Bangladesh_unlabelled.csv")
  banner_sri_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_SriLanka_unlabelled.csv")
  banner_mor_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Morocco_unlabelled.csv")
  banner_ven_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/banner_Venezuela_unlabelled.csv")

  sidebar_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_US.csv")
  sidebar_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_UK.csv")
  sidebar_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Germany.csv")
  sidebar_swe_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Sweden.csv")
  sidebar_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_France.csv")
  sidebar_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Pakistan.csv")
  sidebar_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Bangladesh.csv")
  sidebar_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Sri Lanka.csv")
  sidebar_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Morocco.csv")
  sidebar_ven_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Venezuela.csv")

  sidebar_us_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_US_unlabelled.csv")
  sidebar_uk_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_UK_unlabelled.csv")
  sidebar_ger_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Germany_unlabelled.csv")
  sidebar_swe_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Sweden_unlabelled.csv")
  sidebar_fr_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_France_unlabelled.csv")
  sidebar_pak_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Pak_unlabelled.csv")
  sidebar_ban_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Bangladesh_unlabelled.csv")
  sidebar_sri_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_SriLanka_unlabelled.csv")
  sidebar_mor_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Morocco_unlabelled.csv")
  sidebar_ven_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/sidebar_Venezuela_unlabelled.csv")

  infeed_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_US.csv")
  infeed_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_UK.csv")
  infeed_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Germany.csv")
  infeed_swe_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Sweden.csv")
  infeed_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_France.csv")
  infeed_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Pakistan.csv")
  infeed_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Bangladesh.csv")
  infeed_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Sri Lanka.csv")
  infeed_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Morocco.csv")
  infeed_ven_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Venezuela.csv")

  infeed_us_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_US_unlabelled.csv")
  infeed_uk_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_UK_unlabelled.csv")
  infeed_ger_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Germany_unlabelled.csv")
  infeed_swe_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Sweden_unlabelled.csv")
  infeed_fr_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_France_unlabelled.csv")
  infeed_pak_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Pak_unlabelled.csv")
  infeed_ban_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Bangladesh_unlabelled.csv")
  infeed_sri_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_SriLanka_unlabelled.csv")
  infeed_mor_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Morocco_unlabelled.csv")
  infeed_ven_ul_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/infeed_Venezuela_unlabelled.csv")

  df = pd.DataFrame()
  df['Country'] = ['US_Labelled','UK_Labelled','Ger_Labelled', 'Swe_Labelled','Fr_Labelled','Pak_Labelled','Ban_Labelled','Sri_Labelled','Mor_Labelled','Ven_Labelled', 'US_Unlabelled','UK_Unlabelled','Ger_Unlabelled','Swe_Unlabelled', 'Fr_Unlabelled','Pak_Unlabelled','Ban_Unlabelled','Sri_Unlabelled','Mor_Unlabelled','Ven_Unlabelled']
  df['video ads'] = [len(vid_us_df),len(vid_uk_df),len(vid_ger_df),len(vid_swe_df),len(vid_fr_df),len(vid_pak_df),len(vid_ban_df),len(vid_sri_df),len(vid_mor_df),len(vid_ven_df),len(vid_us_ul_df),len(vid_uk_ul_df),len(vid_ger_ul_df),len(vid_swe_ul_df),len(vid_fr_ul_df),len(vid_pak_ul_df),len(vid_ban_ul_df),len(vid_sri_ul_df),len(vid_mor_ul_df),len(vid_ven_ul_df)]
  df['banner ads']= [len(banner_us_df), len(banner_uk_df), len(banner_ger_df), len(banner_swe_df), len(banner_fr_df), len(banner_pak_df), len(banner_ban_df), len(banner_sri_df), len(banner_mor_df), len(banner_ven_df), len(banner_us_ul_df), len(banner_uk_ul_df), len(banner_ger_ul_df), len(banner_swe_ul_df), len(banner_fr_ul_df), len(banner_pak_ul_df), len(banner_ban_ul_df), len(banner_sri_ul_df), len(banner_mor_ul_df), len(banner_ven_ul_df)]
  df['sidebar ads']=[len(sidebar_us_df), len(sidebar_uk_df), len(sidebar_ger_df), len(sidebar_swe_df), len(sidebar_fr_df), len(sidebar_pak_df), len(sidebar_ban_df), len(sidebar_sri_df), len(sidebar_mor_df), len(sidebar_ven_df), len(sidebar_us_ul_df), len(sidebar_uk_ul_df), len(sidebar_ger_ul_df), len(sidebar_swe_ul_df), len(sidebar_fr_ul_df), len(sidebar_pak_ul_df), len(sidebar_ban_ul_df), len(sidebar_sri_ul_df), len(sidebar_mor_ul_df), len(sidebar_ven_ul_df)]
  df['infeed ads']= [len(infeed_us_df), len(infeed_uk_df), len(infeed_ger_df), len(infeed_swe_df), len(infeed_fr_df), len(infeed_pak_df), len(infeed_ban_df), len(infeed_sri_df), len(infeed_mor_df), len(infeed_ven_df), len(infeed_us_ul_df), len(infeed_uk_ul_df), len(infeed_ger_ul_df), len(infeed_swe_ul_df), len(infeed_fr_ul_df), len(infeed_pak_ul_df), len(infeed_ban_ul_df), len(infeed_sri_ul_df), len(infeed_mor_ul_df), len(infeed_ven_ul_df)]
  #df['Total Ad Count'] = df['Video Ad Count'] + df['Banner Ad Count'] + df['Sidebar Ad Count']

  return df

def prepare_ad_count_df_u_ul():

  #make the initial df:
  df = pd.DataFrame()
  data = {
  'country': ['US', 'UK', 'Germany', 'Sweden', 'France', 'Pakistan', 'Bangladesh', 'Sri Lanka', 'Morocco', 'Venezuela','US', 'UK', 'Germany', 'Sweden', 'France', 'Pakistan', 'Bangladesh', 'Sri Lanka', 'Morocco', 'Venezuela'],
  'region': ['high', 'high', 'high', 'high', 'high', 'low', 'low', 'low', 'low', 'low','high', 'high', 'high', 'high', 'high', 'low', 'low', 'low', 'low', 'low'],
  'type': ['Labelled','Labelled','Labelled','Labelled','Labelled','Labelled','Labelled','Labelled','Labelled','Labelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled','Unlabelled']
  }
  df = pd.DataFrame(data)
  #df.set_index('country', inplace=True)

  vid_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_US.csv")
  vid_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_UK.csv")
  vid_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Germany.csv")
  vid_swe_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sweden.csv")
  vid_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_France.csv")
  vid_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Pakistan.csv")
  vid_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Bangladesh.csv")
  vid_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sri Lanka.csv")
  vid_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Morocco.csv")
  vid_ven_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Venezuela.csv")

  u_vid_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_US_unlabelled.csv")
  u_vid_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_UK_unlabelled.csv")
  u_vid_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Germany_unlabelled.csv")
  u_vid_swe_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Sweden_unlabelled.csv")
  u_vid_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_France_unlabelled.csv")
  u_vid_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Pak_unlabelled.csv")
  u_vid_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Bangladesh_unlabelled.csv")
  u_vid_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_SriLanka_unlabelled.csv")
  u_vid_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Morocco_unlabelled.csv")
  u_vid_ven_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/unique_vid_Venezuela_unlabelled.csv")


  #add count values to the df:
  df['total ad count']=[us_df['total ad count'].sum(),uk_df['total ad count'].sum(),ger_df['total ad count'].sum(),swed_df['total ad count'].sum(),fr_df['total ad count'].sum(),pak_df['total ad count'].sum(),ban_df['total ad count'].sum(),sri_df['total ad count'].sum(),mor_df['total ad count'].sum(),venez_df['total ad count'].sum(),ul_us_df['total ad count'].sum(),ul_uk_df['total ad count'].sum(),ul_ger_df['total ad count'].sum(),ul_swed_df['total ad count'].sum(),ul_fr_df['total ad count'].sum(),ul_pak_df['total ad count'].sum(),ul_ban_df['total ad count'].sum(),ul_sri_df['total ad count'].sum(),ul_mor_df['total ad count'].sum(),ul_venez_df['total ad count'].sum()]
  df['unique ad count']=[len(vid_us_df),len(vid_uk_df),len(vid_ger_df),len(vid_swe_df),len(vid_fr_df),len(vid_pak_df),len(vid_ban_df),len(vid_sri_df),len(vid_mor_df),len(vid_ven_df),len(u_vid_us_df),len(u_vid_uk_df),len(u_vid_ger_df),len(u_vid_swe_df),len(u_vid_fr_df),len(u_vid_pak_df),len(u_vid_ban_df),len(u_vid_sri_df),len(u_vid_mor_df),len(u_vid_ven_df)]

  return df

"""###Preprocessing

"""

###Labelled DFs
pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Pakistan.csv")
sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Sri Lanka.csv")
mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Morocco.csv")
venez_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Venezuela.csv")
ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Bangladesh.csv")
us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/US.csv")
uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/UK.csv")
fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/France.csv")
ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Germany.csv")
swed_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Sweden.csv")

###Unlabelled DFs
ul_pak_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Pakistan_unlabelled.csv")
ul_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/US_unlabelled.csv")
ul_sri_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/SriLanka_unlabelled.csv")
ul_mor_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Morocco_unlabelled.csv")
ul_venez_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Venezuela_unlabelled.csv")
ul_ban_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Bangladesh_unlabelled.csv")
ul_us_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/US_unlabelled.csv")
ul_uk_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/UK_unlabelled.csv")
ul_fr_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/France_unlabelled.csv")
ul_ger_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Germany_unlabelled.csv")
ul_swed_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/Sweden_unlabelled.csv")

###Labelled Ad DFs
pak_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Pakistan.csv")
us_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_US.csv")
sri_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Sri Lanka.csv")
uk_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_UK.csv")
fr_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_France.csv")
ban_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Bangladesh.csv")
mor_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Morocco.csv")
venez_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Venezuela.csv")
ger_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Germany.csv")
swed_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Sweden.csv")

###Unlabelled Ad DFs
ul_pak_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Pak_unlabelled.csv")
ul_us_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_US_unlabelled.csv")
ul_sri_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_SriLanka_unlabelled.csv")
ul_uk_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_UK_unlabelled.csv")
ul_fr_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_France_unlabelled.csv")
ul_ban_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Bangladesh_unlabelled.csv")
ul_mor_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Morocco_unlabelled.csv")
ul_venez_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Venezuela_unlabelled.csv")
ul_ger_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Germany_unlabelled.csv")
ul_swed_ad_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/ad_details_Sweden_unlabelled.csv")

### Add Total Ad Times
pak_df["total ad time"] = addTime(pak_df,pak_ad_df)
us_df["total ad time"] = addTime(us_df,us_ad_df)
uk_df["total ad time"] = addTime(uk_df,uk_ad_df)
sri_df["total ad time"] = addTime(sri_df,sri_ad_df)
ban_df["total ad time"] = addTime(ban_df,ban_ad_df)
fr_df["total ad time"] = addTime(fr_df,fr_ad_df)
ger_df["total ad time"] = addTime(ger_df,ger_ad_df)
swed_df["total ad time"] = addTime(swed_df,swed_ad_df)
mor_df["total ad time"] = addTime(mor_df,mor_ad_df)
venez_df["total ad time"] = addTime(venez_df,venez_ad_df)

ul_pak_df["total ad time"] = addTime(ul_pak_df,ul_pak_ad_df)
ul_us_df["total ad time"] = addTime(ul_us_df,ul_us_ad_df)
ul_uk_df["total ad time"] = addTime(ul_uk_df,ul_uk_ad_df)
ul_sri_df["total ad time"] = addTime(ul_sri_df,ul_sri_ad_df)
ul_ban_df["total ad time"] = addTime(ul_ban_df,ul_ban_ad_df)
ul_fr_df["total ad time"] = addTime(ul_fr_df,ul_fr_ad_df)
ul_ger_df["total ad time"] = addTime(ul_ger_df,ul_ger_ad_df)
ul_swed_df["total ad time"] = addTime(ul_swed_df,ul_swed_ad_df)
ul_mor_df["total ad time"] = addTime(ul_mor_df,ul_mor_ad_df)
ul_venez_df["total ad time"] = addTime(ul_venez_df,ul_venez_ad_df)

### Add Total Ad Counts
pak_df["total ad count"] = addCount(pak_df,pak_ad_df)
us_df["total ad count"] = addCount(us_df,us_ad_df)
uk_df["total ad count"] = addCount(uk_df,uk_ad_df)
sri_df["total ad count"] = addCount(sri_df,sri_ad_df)
ban_df["total ad count"] = addCount(ban_df,ban_ad_df)
fr_df["total ad count"] = addCount(fr_df,fr_ad_df)
mor_df["total ad count"] = addCount(mor_df,mor_ad_df)
venez_df["total ad count"] = addCount(venez_df,venez_ad_df)
swed_df["total ad count"] = addCount(swed_df,swed_ad_df)
ger_df["total ad count"] = addCount(ger_df,ger_ad_df)

ul_pak_df["total ad count"] = addCount(ul_pak_df,ul_pak_ad_df)
ul_us_df["total ad count"] = addCount(ul_us_df,ul_us_ad_df)
ul_uk_df["total ad count"] = addCount(ul_uk_df,ul_uk_ad_df)
ul_sri_df["total ad count"] = addCount(ul_sri_df,ul_sri_ad_df)
ul_ban_df["total ad count"] = addCount(ul_ban_df,ul_ban_ad_df)
ul_fr_df["total ad count"] = addCount(ul_fr_df,ul_fr_ad_df)
ul_mor_df["total ad count"] = addCount(ul_mor_df,ul_mor_ad_df)
ul_venez_df["total ad count"] = addCount(ul_venez_df,ul_venez_ad_df)
ul_swed_df["total ad count"] = addCount(ul_swed_df,ul_swed_ad_df)
ul_ger_df["total ad count"] = addCount(ul_ger_df,ul_ger_ad_df)

### Add Skip Ad Counts
pak_df["skip ad count"] = skippable(pak_df,pak_ad_df)
us_df["skip ad count"] = skippable(us_df,us_ad_df)
uk_df["skip ad count"] = skippable(uk_df,uk_ad_df)
sri_df["skip ad count"] = skippable(sri_df,sri_ad_df)
ban_df["skip ad count"] = skippable(ban_df,ban_ad_df)
fr_df["skip ad count"] = skippable(fr_df,fr_ad_df)
ger_df["skip ad count"] = skippable(ger_df,ger_ad_df)
swed_df["skip ad count"] = skippable(swed_df,swed_ad_df)
mor_df["skip ad count"] = skippable(mor_df,mor_ad_df)
venez_df["skip ad count"] = skippable(venez_df,venez_ad_df)

ul_pak_df["skip ad count"] = skippable(ul_pak_df,ul_pak_ad_df)
ul_us_df["skip ad count"] = skippable(ul_us_df,ul_us_ad_df)
ul_uk_df["skip ad count"] = skippable(ul_uk_df,ul_uk_ad_df)
ul_sri_df["skip ad count"] = skippable(ul_sri_df,ul_sri_ad_df)
ul_ban_df["skip ad count"] = skippable(ul_ban_df,ul_ban_ad_df)
ul_fr_df["skip ad count"] = skippable(ul_fr_df,ul_fr_ad_df)
ul_ger_df["skip ad count"] = skippable(ul_ger_df,ul_ger_ad_df)
ul_swed_df["skip ad count"] = skippable(ul_swed_df,ul_swed_ad_df)
ul_mor_df["skip ad count"] = skippable(ul_mor_df,ul_mor_ad_df)
ul_venez_df["skip ad count"] = skippable(ul_venez_df,ul_venez_ad_df)

### Add Skip Ad Times
pak_df["skip ad time"] = skippableTime(pak_df, pak_ad_df)
us_df["skip ad time"] = skippableTime(us_df, us_ad_df)
uk_df["skip ad time"] = skippableTime(uk_df, uk_ad_df)
sri_df["skip ad time"] = skippableTime(sri_df, sri_ad_df)
ban_df["skip ad time"] = skippableTime(ban_df, ban_ad_df)
fr_df["skip ad time"] = skippableTime(fr_df, fr_ad_df)
ger_df["skip ad time"] = skippableTime(ger_df, ger_ad_df)
swed_df["skip ad time"] = skippableTime(swed_df, swed_ad_df)
mor_df["skip ad time"] = skippableTime(mor_df, mor_ad_df)
venez_df["skip ad time"] = skippableTime(venez_df, venez_ad_df)

ul_pak_df["skip ad time"] = skippableTime(ul_pak_df, ul_pak_ad_df)
ul_us_df["skip ad time"] = skippableTime(ul_us_df, ul_us_ad_df)
ul_uk_df["skip ad time"] = skippableTime(ul_uk_df, ul_uk_ad_df)
ul_sri_df["skip ad time"] = skippableTime(ul_sri_df, ul_sri_ad_df)
ul_ban_df["skip ad time"] = skippableTime(ul_ban_df, ul_ban_ad_df)
ul_fr_df["skip ad time"] = skippableTime(ul_fr_df, ul_fr_ad_df)
ul_ger_df["skip ad time"] = skippableTime(ul_ger_df, ul_ger_ad_df)
ul_swed_df["skip ad time"] = skippableTime(ul_swed_df, ul_swed_ad_df)
ul_mor_df["skip ad time"] = skippableTime(ul_mor_df, ul_mor_ad_df)
ul_venez_df["skip ad time"] = skippableTime(ul_venez_df, ul_venez_ad_df)

### Low Policy Labelled Skippable Durations
pak_skips = getSkippableDurations(pak_df,pak_ad_df)
ban_skips = getSkippableDurations(ban_df,ban_ad_df)
sri_skips = getSkippableDurations(sri_df,sri_ad_df)
mor_skips = getSkippableDurations(mor_df,mor_ad_df)
venez_skips = getSkippableDurations(venez_df,venez_ad_df)

pak_skips['pol'] = ['Low' for i in range(len(pak_skips['ad']))]
ban_skips['pol'] = ['Low' for i in range(len(ban_skips['ad']))]
sri_skips['pol'] = ['Low' for i in range(len(sri_skips['ad']))]
mor_skips['pol'] = ['Low' for i in range(len(mor_skips['ad']))]
venez_skips['pol'] = ['Low' for i in range(len(venez_skips['ad']))]

pak_skips['country'] = ['Pakistan' for i in range(len(pak_skips['ad']))]
ban_skips['country'] = ['Bangladesh' for i in range(len(ban_skips['ad']))]
sri_skips['country'] = ['Sri Lanka' for i in range(len(sri_skips['ad']))]
mor_skips['country'] = ['Morocco' for i in range(len(mor_skips['ad']))]
venez_skips['country'] = ['Venezuela' for i in range(len(venez_skips['ad']))]

### Low Policy Unlabelled Skippable Durations
pak_ul_skips = getSkippableDurations(ul_pak_df,ul_pak_ad_df)
ban_ul_skips = getSkippableDurations(ul_ban_df,ul_ban_ad_df)
sri_ul_skips = getSkippableDurations(ul_sri_df,ul_sri_ad_df)
mor_ul_skips = getSkippableDurations(ul_mor_df,ul_mor_ad_df)
venez_ul_skips = getSkippableDurations(ul_venez_df,ul_venez_ad_df)

pak_ul_skips['pol'] = ['Low UL' for i in range(len(pak_ul_skips['ad']))]
ban_ul_skips['pol'] = ['Low UL' for i in range(len(ban_ul_skips['ad']))]
sri_ul_skips['pol'] = ['Low UL' for i in range(len(sri_ul_skips['ad']))]
mor_ul_skips['pol'] = ['Low UL' for i in range(len(mor_ul_skips['ad']))]
venez_ul_skips['pol'] = ['Low UL' for i in range(len(venez_ul_skips['ad']))]

pak_ul_skips['country'] = ['Pakistan' for i in range(len(pak_ul_skips['ad']))]
ban_ul_skips['country'] = ['Bangladesh' for i in range(len(ban_ul_skips['ad']))]
sri_ul_skips['country'] = ['Sri Lanka' for i in range(len(sri_ul_skips['ad']))]
mor_ul_skips['country'] = ['Morocco' for i in range(len(mor_ul_skips['ad']))]
venez_ul_skips['country'] = ['Venezuela' for i in range(len(venez_ul_skips['ad']))]

### High Policy Labelled Skippable Durations
us_skips = getSkippableDurations(us_df,us_ad_df)
uk_skips = getSkippableDurations(uk_df,uk_ad_df)
ger_skips = getSkippableDurations(ger_df,ger_ad_df)
swed_skips = getSkippableDurations(swed_df,swed_ad_df)
fr_skips = getSkippableDurations(fr_df,fr_ad_df)

us_skips['pol'] = ['High' for i in range(len(us_skips['ad']))]
uk_skips['pol'] = ['High' for i in range(len(uk_skips['ad']))]
ger_skips['pol'] = ['High' for i in range(len(ger_skips['ad']))]
swed_skips['pol'] = ['High' for i in range(len(swed_skips['ad']))]
fr_skips['pol'] = ['High' for i in range(len(fr_skips['ad']))]

us_skips['country'] = ['US' for i in range(len(us_skips['ad']))]
uk_skips['country'] = ['UK' for i in range(len(uk_skips['ad']))]
ger_skips['country'] = ['Germany' for i in range(len(ger_skips['ad']))]
swed_skips['country'] = ['Sweden' for i in range(len(swed_skips['ad']))]
fr_skips['country'] = ['France' for i in range(len(fr_skips['ad']))]

### High Policy Unlabelled Skippable Durations
us_ul_skips = getSkippableDurations(ul_us_df,ul_us_ad_df)
uk_ul_skips = getSkippableDurations(ul_uk_df,ul_uk_ad_df)
ger_ul_skips = getSkippableDurations(ul_ger_df,ul_ger_ad_df)
swed_ul_skips = getSkippableDurations(ul_swed_df,ul_swed_ad_df)
fr_ul_skips = getSkippableDurations(ul_fr_df,ul_fr_ad_df)

us_ul_skips['pol'] = ['High UL' for i in range(len(us_ul_skips['ad']))]
uk_ul_skips['pol'] = ['High UL' for i in range(len(uk_ul_skips['ad']))]
ger_ul_skips['pol'] = ['High UL' for i in range(len(ger_ul_skips['ad']))]
swed_ul_skips['pol'] = ['High UL' for i in range(len(swed_ul_skips['ad']))]
fr_ul_skips['pol'] = ['High UL' for i in range(len(fr_ul_skips['ad']))]

us_ul_skips['country'] = ['US' for i in range(len(us_ul_skips['ad']))]
uk_ul_skips['country'] = ['UK' for i in range(len(uk_ul_skips['ad']))]
ger_ul_skips['country'] = ['Germany' for i in range(len(ger_ul_skips['ad']))]
swed_ul_skips['country'] = ['Sweden' for i in range(len(swed_ul_skips['ad']))]
fr_ul_skips['country'] = ['France' for i in range(len(fr_ul_skips['ad']))]

### Low Policy Labelled Unskippable Durations
pak_uskips = getUnSkippableDurations(pak_df,pak_ad_df)
ban_uskips = getUnSkippableDurations(ban_df,ban_ad_df)
sri_uskips = getUnSkippableDurations(sri_df,sri_ad_df)
mor_uskips = getUnSkippableDurations(mor_df,mor_ad_df)
venez_uskips = getUnSkippableDurations(venez_df,venez_ad_df)

pak_uskips['country'] = ['Pakistan' for i in range(len(pak_uskips['ad']))]
ban_uskips['country'] = ['Bangladesh' for i in range(len(ban_uskips['ad']))]
sri_uskips['country'] = ['Sri Lanka' for i in range(len(sri_uskips['ad']))]
mor_uskips['country'] = ['Morocco' for i in range(len(mor_uskips['ad']))]
venez_uskips['country'] = ['Venezuela' for i in range(len(venez_uskips['ad']))]

pak_uskips['pol'] = ['Low' for i in range(len(pak_uskips['ad']))]
ban_uskips['pol'] = ['Low' for i in range(len(ban_uskips['ad']))]
sri_uskips['pol'] = ['Low' for i in range(len(sri_uskips['ad']))]
mor_uskips['pol'] = ['Low' for i in range(len(mor_uskips['ad']))]
venez_uskips['pol'] = ['Low' for i in range(len(venez_uskips['ad']))]

### Low Policy Unlabelled Unskippable Durations
pak_ul_uskips = getUnSkippableDurations(ul_pak_df,ul_pak_ad_df)
ban_ul_uskips = getUnSkippableDurations(ul_ban_df,ul_ban_ad_df)
sri_ul_uskips = getUnSkippableDurations(ul_sri_df,ul_sri_ad_df)
mor_ul_uskips = getUnSkippableDurations(ul_mor_df,ul_mor_ad_df)
venez_ul_uskips = getUnSkippableDurations(ul_venez_df,ul_venez_ad_df)

pak_ul_uskips['pol'] = ['Low UL' for i in range(len(pak_ul_uskips['ad']))]
ban_ul_uskips['pol'] = ['Low UL' for i in range(len(ban_ul_uskips['ad']))]
sri_ul_uskips['pol'] = ['Low UL' for i in range(len(sri_ul_uskips['ad']))]
mor_ul_uskips['pol'] = ['Low UL' for i in range(len(mor_ul_uskips['ad']))]
venez_ul_uskips['pol'] = ['Low UL' for i in range(len(venez_ul_uskips['ad']))]

pak_ul_uskips['country'] = ['Pakistan' for i in range(len(pak_ul_uskips['ad']))]
ban_ul_uskips['country'] = ['Bangladesh' for i in range(len(ban_ul_uskips['ad']))]
sri_ul_uskips['country'] = ['Sri Lanka' for i in range(len(sri_ul_uskips['ad']))]
mor_ul_uskips['country'] = ['Morocco' for i in range(len(mor_ul_uskips['ad']))]
venez_ul_uskips['country'] = ['Venezuela' for i in range(len(venez_ul_uskips['ad']))]

### High Policy Labelled Unskippable Durations
us_uskips = getUnSkippableDurations(us_df,us_ad_df)
uk_uskips = getUnSkippableDurations(uk_df,uk_ad_df)
ger_uskips = getUnSkippableDurations(ger_df,ger_ad_df)
swed_uskips = getUnSkippableDurations(swed_df,swed_ad_df)
fr_uskips = getUnSkippableDurations(fr_df,fr_ad_df)

us_uskips['pol'] = ['High' for i in range(len(us_uskips['ad']))]
uk_uskips['pol'] = ['High' for i in range(len(uk_uskips['ad']))]
ger_uskips['pol'] = ['High' for i in range(len(ger_uskips['ad']))]
swed_uskips['pol'] = ['High' for i in range(len(swed_uskips['ad']))]
fr_uskips['pol'] = ['High' for i in range(len(fr_uskips['ad']))]
us_uskips['country'] = ['US' for i in range(len(us_uskips['ad']))]
uk_uskips['country'] = ['UK' for i in range(len(uk_uskips['ad']))]
ger_uskips['country'] = ['Germany' for i in range(len(ger_uskips['ad']))]
swed_uskips['country'] = ['Sweden' for i in range(len(swed_uskips['ad']))]
fr_uskips['country'] = ['France' for i in range(len(fr_uskips['ad']))]


### High Policy Unlabelled Unskippable Durations
us_ul_uskips = getUnSkippableDurations(ul_us_df,ul_us_ad_df)
uk_ul_uskips = getUnSkippableDurations(ul_uk_df,ul_uk_ad_df)
ger_ul_uskips = getUnSkippableDurations(ul_ger_df,ul_ger_ad_df)
swed_ul_uskips = getUnSkippableDurations(ul_swed_df,ul_swed_ad_df)
fr_ul_uskips = getUnSkippableDurations(ul_fr_df,ul_fr_ad_df)

us_ul_uskips['pol'] = ['High UL' for i in range(len(us_ul_uskips['ad']))]
uk_ul_uskips['pol'] = ['High UL' for i in range(len(uk_ul_uskips['ad']))]
ger_ul_uskips['pol'] = ['High UL' for i in range(len(ger_ul_uskips['ad']))]
swed_ul_uskips['pol'] = ['High UL' for i in range(len(swed_ul_uskips['ad']))]
fr_ul_uskips['pol'] = ['High UL' for i in range(len(fr_ul_uskips['ad']))]

us_ul_uskips['country'] = ['US' for i in range(len(us_ul_uskips['ad']))]
uk_ul_uskips['country'] = ['UK' for i in range(len(uk_ul_uskips['ad']))]
ger_ul_uskips['country'] = ['Germany' for i in range(len(ger_ul_uskips['ad']))]
swed_ul_uskips['country'] = ['Sweden' for i in range(len(swed_ul_uskips['ad']))]
fr_ul_uskips['country'] = ['France' for i in range(len(fr_ul_uskips['ad']))]

### Get distributions:

us_dist = getDist(us_df, "US")
uk_dist = getDist(uk_df, "UK")
fr_dist = getDist(fr_df, "France")
ger_dist = getDist(ger_df, "Germany")
swed_dist = getDist(swed_df, "Sweden")
#LABELLED HIGH POL: reg_dist
reg_dist=pd.concat([us_dist, uk_dist, fr_dist, ger_dist, swed_dist], axis=0)
reg_dist.reset_index(level=0, inplace=True)

us_ul_dist = getDist(ul_us_df, "US")
uk_ul_dist = getDist(ul_uk_df, "UK")
fr_ul_dist = getDist(ul_fr_df, "France")
ger_ul_dist = getDist(ul_ger_df, "Germany")
swed_ul_dist = getDist(ul_swed_df, "Sweden")
#UNLABELLED HIGH POL: ureg_dist
ureg_dist=pd.concat([us_ul_dist, uk_ul_dist, fr_ul_dist, ger_ul_dist, swed_ul_dist], axis=0)
ureg_dist.reset_index(level=0, inplace=True)

pak_dist = getDist(pak_df, "Pakistan")
ban_dist = getDist(ban_df, "Bangladesh")
sri_dist = getDist(sri_df, "Sri Lanka")
mor_dist = getDist(mor_df, "Morocco")
ven_dist = getDist(venez_df, "Venezuela")
#LABELLED LOW POL: nreg_dist
nreg_dist=pd.concat([pak_dist, ban_dist, sri_dist, mor_dist, ven_dist], axis=0)
nreg_dist.reset_index(level=0, inplace=True)

pak_ul_dist = getDist(ul_pak_df, "Pakistan")
ban_ul_dist = getDist(ul_ban_df, "Bangladesh")
sri_ul_dist = getDist(ul_sri_df, "Sri Lanka")
mor_ul_dist = getDist(ul_mor_df, "Morocco")
venez_ul_dist = getDist(ul_venez_df, "Venezuela")
#UNLABELLED LOW POL: unreg_dist
unreg_dist=pd.concat([pak_ul_dist, ban_ul_dist, sri_ul_dist, mor_ul_dist, venez_ul_dist], axis=0)
unreg_dist.reset_index(level=0, inplace=True)


reg_dist['region']=['high policy' for i in range(len(reg_dist))]
nreg_dist['region']=['low policy' for i in range(len(nreg_dist))]
combined_dist = pd.concat([reg_dist,nreg_dist],axis=0)

ureg_dist['region']=['high policy' for i in range(len(ureg_dist))]
unreg_dist['region']=['low policy' for i in range(len(unreg_dist))]
ucombined_dist = pd.concat([ureg_dist,unreg_dist],axis=0)

combined_dist['label']=['yes' for i in range(len(combined_dist))]
ucombined_dist['label']=['no' for i in range(len(ucombined_dist))]

f_dist = pd.concat([combined_dist,ucombined_dist],axis=0)

f_dist.groupby("label").mean()

low_pol_dist = combined_dist[combined_dist['region'] == 'low policy']
high_pol_dist = combined_dist[combined_dist['region'] == 'high policy']
ulow_pol_dist = ucombined_dist[ucombined_dist['region'] == 'low policy']
uhigh_pol_dist = ucombined_dist[ucombined_dist['region'] == 'high policy']

us_times = getTimeDF(us_df)
uk_times = getTimeDF(uk_df)
sri_times = getTimeDF(sri_df)
ban_times = getTimeDF(ban_df)
fr_times = getTimeDF(fr_df)
pak_times = getTimeDF(pak_df)
ger_times = getTimeDF(ger_df)
swed_times = getTimeDF(swed_df)
mor_times = getTimeDF(mor_df)
venez_times = getTimeDF(venez_df)

uk_times['country'] = ['UK' for i in range(len(uk_times['ad time']))]
us_times['country'] = ['US' for i in range(len(us_times['ad time']))]
fr_times['country'] = ['France' for i in range(len(fr_times['ad time']))]
sri_times['country'] = ['Sri Lanka' for i in range(len(sri_times['ad time']))]
ban_times['country'] = ['Bangladesh' for i in range(len(ban_times['ad time']))]
pak_times['country'] = ['Pakistan' for i in range(len(pak_times['ad time']))]
ger_times['country'] = ['Germany' for i in range(len(ger_times['ad time']))]
swed_times['country'] = ['Sweden' for i in range(len(swed_times['ad time']))]
mor_times['country'] = ['Morocco' for i in range(len(mor_times['ad time']))]
venez_times['country'] = ['Venezuela' for i in range(len(venez_times['ad time']))]

uk_times['segment'] = ['H.P Labelled' for i in range(len(uk_times['ad time']))]
us_times['segment'] = ['H.P Labelled' for i in range(len(us_times['ad time']))]
fr_times['segment'] = ['H.P Labelled' for i in range(len(fr_times['ad time']))]
sri_times['segment'] = ['L.P Labelled' for i in range(len(sri_times['ad time']))]
ban_times['segment'] = ['L.P Labelled' for i in range(len(ban_times['ad time']))]
mor_times['segment'] = ['L.P Labelled' for i in range(len(mor_times['ad time']))]
venez_times['segment'] = ['L.P Labelled' for i in range(len(venez_times['ad time']))]
ger_times['segment'] = ['H.P Labelled' for i in range(len(ger_times['ad time']))]
swed_times['segment'] = ['H.P Labelled' for i in range(len(swed_times['ad time']))]
pak_times['segment'] = ['L.P Labelled' for i in range(len(pak_times['ad time']))]

reg_regional_cdf=pd.concat([uk_times, us_times, fr_times, ger_times, swed_times], axis=0)
nreg_regional_cdf=pd.concat([sri_times, ban_times, pak_times,mor_times, venez_times], axis=0)

us_ul_times = getTimeDF(ul_us_df)
uk_ul_times = getTimeDF(ul_uk_df)
sri_ul_times = getTimeDF(ul_sri_df)
ban_ul_times = getTimeDF(ul_ban_df)
fr_ul_times = getTimeDF(ul_fr_df)
pak_ul_times = getTimeDF(ul_pak_df)
ger_ul_times = getTimeDF(ul_ger_df)
swed_ul_times = getTimeDF(ul_swed_df)
mor_ul_times = getTimeDF(ul_mor_df)
venez_ul_times = getTimeDF(ul_venez_df)

uk_ul_times['country'] = ['UK' for i in range(len(uk_ul_times['ad time']))]
us_ul_times['country'] = ['US' for i in range(len(us_ul_times['ad time']))]
fr_ul_times['country'] = ['France' for i in range(len(fr_ul_times['ad time']))]
sri_ul_times['country'] = ['Sri Lanka' for i in range(len(sri_ul_times['ad time']))]
ban_ul_times['country'] = ['Bangladesh' for i in range(len(ban_ul_times['ad time']))]
pak_ul_times['country'] = ['Pakistan' for i in range(len(pak_ul_times['ad time']))]
ger_ul_times['country'] = ['Germany' for i in range(len(ger_ul_times['ad time']))]
swed_ul_times['country'] = ['Sweden' for i in range(len(swed_ul_times['ad time']))]
mor_ul_times['country'] = ['Morocco' for i in range(len(mor_ul_times['ad time']))]
venez_ul_times['country'] = ['Venezuela' for i in range(len(venez_ul_times['ad time']))]


uk_ul_times['segment'] = ['H.P Unlabelled' for i in range(len(uk_ul_times['ad time']))]
us_ul_times['segment'] = ['H.P Unlabelled' for i in range(len(us_ul_times['ad time']))]
fr_ul_times['segment'] = ['H.P Unlabelled' for i in range(len(fr_ul_times['ad time']))]
sri_ul_times['segment'] = ['L.P Unlabelled' for i in range(len(sri_ul_times['ad time']))]
ban_ul_times['segment'] = ['L.P Unlabelled' for i in range(len(ban_ul_times['ad time']))]
mor_ul_times['segment'] = ['L.P Unlabelled' for i in range(len(mor_ul_times['ad time']))]
venez_ul_times['segment'] = ['L.P Unlabelled' for i in range(len(venez_ul_times['ad time']))]
ger_ul_times['segment'] = ['H.P Unlabelled' for i in range(len(ger_ul_times['ad time']))]
swed_ul_times['segment'] = ['H.P Unlabelled' for i in range(len(swed_ul_times['ad time']))]
pak_ul_times['segment'] = ['L.P Unlabelled' for i in range(len(pak_ul_times['ad time']))]

u_reg_regional_cdf=pd.concat([uk_ul_times, us_ul_times, fr_ul_times, ger_ul_times, swed_ul_times], axis=0)
u_nreg_regional_cdf=pd.concat([sri_ul_times, ban_ul_times, pak_ul_times,mor_ul_times, venez_ul_times], axis=0)

u_reg_regional_cdf.reset_index(level=0, inplace=True)
u_nreg_regional_cdf.reset_index(level=0, inplace=True)
reg_regional_cdf.reset_index(level=0, inplace=True)
nreg_regional_cdf.reset_index(level=0, inplace=True)

comb_cdf = pd.concat([reg_regional_cdf,nreg_regional_cdf],  axis=0)
comb_cdf.reset_index(level=0, inplace=True)

ul_comb_cdf = pd.concat([u_reg_regional_cdf,u_nreg_regional_cdf],  axis=0)
ul_comb_cdf.reset_index(level=0, inplace=True)

f_cdf = pd.concat([u_reg_regional_cdf,u_nreg_regional_cdf,reg_regional_cdf,nreg_regional_cdf],  axis=0)
f_cdf.reset_index(level=0, inplace=True)

pol = pd.concat([pak_skips,ban_skips,sri_skips,mor_skips,venez_skips,us_skips ,uk_skips,ger_skips,swed_skips,fr_skips])
pol.reset_index(level=0, inplace=True)

ul = pd.concat([pak_ul_skips,ban_ul_skips,sri_ul_skips,mor_ul_skips,venez_ul_skips,us_ul_skips ,uk_ul_skips,ger_ul_skips,swed_ul_skips,fr_ul_skips])
ul.reset_index(level=0, inplace=True)

l_ul = pd.concat([ul, pol])
l_ul.reset_index(level=0, inplace=True)

upol = pd.concat([pak_uskips,ban_uskips,sri_uskips,mor_uskips,venez_uskips,us_uskips ,uk_uskips,ger_uskips,swed_uskips,fr_uskips])
upol.reset_index(level=0, inplace=True)

ul_unskip = pd.concat([pak_ul_uskips,ban_ul_uskips,sri_ul_uskips,mor_ul_uskips,venez_ul_uskips,us_ul_uskips ,uk_ul_uskips,ger_ul_uskips,swed_ul_uskips,fr_ul_uskips])
ul_unskip.reset_index(level=0, inplace=True)

l_ul_unskip = pd.concat([ul_unskip, upol])
l_ul_unskip.reset_index(level=0, inplace=True)

agg_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/aggPrimaryTags.csv")
GREEN = sns.color_palette("Set3")[0]
RED = sns.color_palette("Set3")[3]
labelled_agg = agg_df[agg_df['Content']=="Labelled"].copy()
labelled_agg.drop(columns='Content',inplace=True)
melted_agg1 = pd.melt(labelled_agg, id_vars=["Region"], var_name="category", value_name="value")
melted_agg1['proportion'] = melted_agg1['value']/750

inapp_rating_df = pd.read_csv("drive/My Drive/Topics in IR Project/data_csvs/17May/aggRatings.csv")
inapp_rating_df = inapp_rating_df[inapp_rating_df['Content']=="Labelled"].copy()
inapp_rating_df['Mild']=inapp_rating_df['Mild']/inapp_rating_df['Total']
inapp_rating_df['Moderate']=inapp_rating_df['Moderate']/inapp_rating_df['Total']
inapp_rating_df['Extreme']=inapp_rating_df['Extreme']/inapp_rating_df['Total']
inapp_rating_df.drop(columns=['Content','Total'],inplace=True)
melted_inapp_rating_df = pd.melt(inapp_rating_df, id_vars=["Region"], var_name="category", value_name="value")

## aggregating dataframes for countries
pol = pd.concat([pak_skips,ban_skips,sri_skips,mor_skips,venez_skips,us_skips ,uk_skips,ger_skips,swed_skips,fr_skips])
pol.reset_index(level=0, inplace=True)

## aggregating dataframes for labelled and unlabelled comparisons
ul = pd.concat([pak_skips, pak_ul_skips, us_skips, us_ul_skips])
ul.reset_index(level=0, inplace=True)

ulu = pd.concat([pak_uskips, pak_ul_uskips, us_uskips, us_ul_uskips])
ulu.reset_index(level=0, inplace=True)

upol = pd.concat([pak_uskips,ban_uskips,sri_uskips,mor_uskips,venez_uskips,us_uskips ,uk_uskips,ger_uskips,swed_uskips,fr_uskips])
upol.reset_index(level=0, inplace=True)

mean_high = pol[pol['pol'] == 'High']["duration"].mean()

mean_low = pol[pol['pol'] == 'Low']["duration"].mean()
umean_high = upol[upol['pol'] == 'High']["duration"].mean()

umean_low = upol[upol['pol'] == 'Low']["duration"].mean()
pol.loc[pol['duration'] == 5326.0, 'country'].iloc[0]

ul[ul['country'] == 'Pakistan UL']["duration"].max()

upol[upol["pol"] == "High"].max()
# upol.loc[upol['duration'] == 6323.0, 'country'].iloc[0]

# percentiles = np.nanpercentile(ul[ul["country"] == "Pakistan UL"]["duration"], [25, 50, 75, 90])

# print("25th percentile:", percentiles[0])
# print("50th percentile (median):", percentiles[1])
# print("75th percentile:", percentiles[2])
# print("90th percentile:", percentiles[3])

"""### Figure 2:

(a) CDF of skippable ad durations across both high and low policy regions.
(b) CDF of unskippable ad durations across both high and low policy regions.
"""

### Figure 3(a)
fig = plt.figure(figsize=(6, 4))
sns.set_style("white")
plt.grid(axis='y', linestyle='dashed')
ax = sns.ecdfplot(data=pol[pol["duration"] < 1500],x="duration", marker='',hue="pol",ls='solid',  palette=[sns.color_palette("Set3")[3],sns.color_palette("Set3")[0]],linewidth=2)
# Extract the legend information from the seaborn plot
legend = ax.get_legend()

# Get the legend handles and labels
legend_handles, legend_labels = legend.legendHandles, [text.get_text() for text in legend.get_texts()]

plt.axvline(360, color="black", linestyle='--', label="YouTube Ad Limit", linewidth=1, alpha=0.7)
plt.axvline(60, color="maroon", linestyle='--', label="YT Kids Ad Limit",linewidth=1, alpha=0.7)
plt_handles, plt_labels = plt.gca().get_legend_handles_labels()
new_labels = ['Low Policy Regions', 'High Policy Regions']

combined_handles = legend_handles + plt_handles
combined_labels = new_labels + plt_labels
plt.legend(combined_handles, combined_labels)
lines = plt.gca().lines

# Specify the x-coordinate of the vertical line
vertical_line_x = 60

# Initialize variables to store the y-coordinates of the intersection points
y_intersect_A = None
y_intersect_B = None

# Iterate through the lines to find the curves of interest
for line in lines:
    if line.get_label() == '_child0':
        # Interpolate the y-value of curve A at the specified x-coordinate
        y_intersect_A = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())
    elif line.get_label() == '_child1':
        # Interpolate the y-value of curve B at the specified x-coordinate
        y_intersect_B = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())


# # Plot the intersection points
plt.plot(vertical_line_x, y_intersect_A, 'o', color="maroon", label='Intersection A', markersize=3)
plt.plot(vertical_line_x, y_intersect_B, 'o', color="maroon", label='Intersection B',markersize=3)
#print(y_intersect_A, y_intersect_B)
print("Percentage of skippable ads falling under the YouTube Kids limit for ad durations in High Policy Regions: ",round(y_intersect_A*100,1),'%')
print("Percentage of skippable ads falling under the YouTube Kids limit for ad durations in Low Policy Regions: ",round(y_intersect_B*100,1),'%')

plt.text(60+10, y_intersect_A+0.02, f"{round(y_intersect_A*100,1)}%", ha='left', va='center_baseline', color='black', fontsize=12)
plt.text(60+10, y_intersect_B+0.02, f"{round(y_intersect_B*100,1)}%", ha='left', va='center_baseline', color='black', fontsize=12)

plt.tick_params(axis='x',labelsize=13)
plt.tick_params(axis='y',labelsize=13)
plt.xlabel("Ad Duration (seconds)",fontsize=14)
plt.ylabel("Proportion",fontsize=14)

print("Figure 3(a)")
plt.show()

u_stat, p_value = stats.mannwhitneyu(pol[pol["pol"] == "High"]["duration"].dropna(),pol[pol["pol"] == "Low"]["duration"].dropna())
print("Mann Whitney U-test p-value comparing ad durations for the unique skippable ads across high and low policy regions:", p_value)

skippableLowPolicy = pd.concat([pak_skips,ban_skips,sri_skips,mor_skips,venez_skips])
skippableHighPolicy = pd.concat([us_skips ,uk_skips,ger_skips,swed_skips,fr_skips])
print("Median duration in skippable ad pool for High Policy regions: ",round(skippableLowPolicy['duration'].median(),0),"secs")
print("Median duration in skippable ad pool for Low Policy regions: ",round(skippableHighPolicy['duration'].median(),0),"secs")

print("Maxmium outlier of ad durations: ",round(((pol['duration'].max()/60)/60),1),"hours")

### Figure 3(b)
fig = plt.figure(figsize=(6, 4))
sns.set_style("white")
plt.grid(axis='y', linestyle='dashed')
ax = sns.ecdfplot(data=upol[upol["duration"] < 1500],x="duration", marker='',hue="pol",ls='solid',  palette=[sns.color_palette("Set3")[3],sns.color_palette("Set3")[0]],linewidth=2)
# Extract the legend information from the seaborn plot
legend = ax.get_legend()

# Get the legend handles and labels
legend_handles, legend_labels = legend.legendHandles, [text.get_text() for text in legend.get_texts()]

plt.axvline(15, color="maroon", linestyle='--', label="YouTube + YT Kids Ad Limit", linewidth=1, alpha=0.9)
plt_handles, plt_labels = plt.gca().get_legend_handles_labels()
new_labels = ['Low Policy Regions', 'High Policy Regions']

combined_handles = legend_handles + plt_handles
combined_labels = new_labels + plt_labels
plt.legend(combined_handles, combined_labels)

lines = plt.gca().lines

# Specify the x-coordinate of the vertical line
vertical_line_x = 16

# Initialize variables to store the y-coordinates of the intersection points
y_intersect_A = None
y_intersect_B = None

# Iterate through the lines to find the curves of interest
for line in lines:

    if line.get_label() == '_child0':
        # Interpolate the y-value of curve A at the specified x-coordinate
        y_intersect_A = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())
    elif line.get_label() == '_child1':
        # Interpolate the y-value of curve B at the specified x-coordinate
        y_intersect_B = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())


# # Plot the intersection points
plt.plot(vertical_line_x, y_intersect_A, 'o', color="maroon", label='Intersection A', markersize=4)
plt.plot(vertical_line_x, y_intersect_B, 'o', color="maroon", label='Intersection B', markersize=4)
print(y_intersect_A, y_intersect_B)
print("Percentage of Unskippable ads in High Policy Regions below the 15s limit: ",round(y_intersect_A*100,1),'%')
print("Percentage of Unskippable ads in Low Policy Regions below the 15s limit: ",round(y_intersect_B*100,1),'%')
print("Percentage of Unskippable ads in High Policy Regions above the 15s limit: ",round((1-y_intersect_A)*100,1),'%')
print("Percentage of Unskippable ads in Low Policy Regions above the 15s limit: ",round((1-y_intersect_B)*100,1),'%')
plt.text(60, y_intersect_A+0.02, f"{round(y_intersect_A*100,1)}%", ha='left', va='center_baseline', color='black', fontsize=12)
plt.text(60, y_intersect_B+0.02, f"{round(y_intersect_B*100,1)}%", ha='left', va='center_baseline', color='black', fontsize=12)

plt.tick_params(axis='x',labelsize=13)
plt.tick_params(axis='y',labelsize=13)
plt.xlabel("Ad Duration (seconds)",fontsize=14)
plt.ylabel("Proportion",fontsize=14)
print("Figure 3(b)")

statistic, p_value = stats.ttest_ind(upol[upol["pol"] == "High"]["duration"].dropna(),upol[upol["pol"] == "Low"]["duration"].dropna())
print("Welch t-test p-value comparing ad durations for the unique unskippable ads across high and low policy regions:", p_value)

print("Maximum duration of an unskippable ad: ",round((((upol['duration'].max())/60)/60),1),'hours')

print("Mean unskippable ad length in High Policy regions: ",round(upol[upol["pol"] == "High"]["duration"].mean(),0),"secs")
print("Mean unskippable ad length in High Policy regions: ",round(upol[upol["pol"] == "Low"]["duration"].mean(),0),"secs")

statistic, p_value = stats.ttest_ind(low_pol_dist[low_pol_dist["region"] == "low policy"]["num of ads"], high_pol_dist[high_pol_dist["region"] == "high policy"]["num of ads"])
print("Welch t-test p-value comparing ad frequency across high and low policy regions:", p_value)

statistic, p_value = stats.ttest_ind(comb_cdf[comb_cdf["segment"] == "H.P Labelled"]["ad time"].dropna(), comb_cdf[comb_cdf["segment"] == "L.P Labelled"]["ad time"].dropna())
print("Mann Whitney U-Test p-value comparing cumulative ad duration per video across high and low policy regions:", p_value)

statistic, p_value = stats.mannwhitneyu(comb_cdf[comb_cdf["segment"] == "H.P Labelled"]["ad time"].dropna(), comb_cdf[comb_cdf["segment"] == "L.P Labelled"]["ad time"].dropna())
print("Welch t-test p-value comparing cumulative ad duration per video across high and low policy regions:", p_value)

print("Mean number of ads per video in low policy areas: ",round(high_labelled_VS_low_labelled_mean_df[high_labelled_VS_low_labelled_mean_df['region']=='Low Policy']['mean'].values[0],1))
print("Mean number of ads per video in High policy areas: ",round(high_labelled_VS_low_labelled_mean_df[high_labelled_VS_low_labelled_mean_df['region']=='High Policy']['mean'].values[0],1))

print("Mean ad time of Pakistan: ",round(comb_cdf[(comb_cdf['country']=='Pakistan')]['ad time'].mean(),0),"secs")
print("Max outlier value: ",round((((comb_cdf[(comb_cdf['country']=='Pakistan')]['ad time'].max())/60)/60),1),"hours")

"""### Figure 3:

(a) Mean ad frequency across high and low policy regions. Error bars show 95% confidence intervals.
(b) Ad frequency per video across high policy and low policy regions.
"""

values_low_pol,values_high_pol,five_plus_low_pol,five_plus_high_pol=histobar_maker(low_pol_dist,high_pol_dist)

comb_cdf["ratio"] = comb_cdf["ad time"]/comb_cdf["video time"]

from matplotlib.patches import Patch

fig = plt.figure(figsize=(6, 3))
plt.grid(axis='y', linestyle='dashed')
custom_palette = [sns.color_palette("Set3")[0] for i in range(5)] + [sns.color_palette("Set3")[3] for i in range(5)]
sns.boxplot(x="country", y="ad time", data=comb_cdf[comb_cdf["ad time"] < 1500], whis=2.5, palette=custom_palette)
plt.tick_params(axis='x', labelsize=13)
plt.tick_params(axis='y', labelsize=13)

# Country codes instead of names
country_codes =  ['US', 'GB', 'FR', 'DE', 'SE', 'LK', 'BD', 'PK', 'MA', 'VE']

labels = ['High Policy', 'Low Policy']
colors = [sns.color_palette("Set3")[0], sns.color_palette("Set3")[3]]
legend_patches = [Patch(color=color, label=label) for color, label in zip(colors, labels)]
plt.legend(handles=legend_patches, loc="upper right")

# Increase the size of x-axis label and y-axis label
plt.xlabel("Country", fontsize=14)
# plt.tick_params(axis='x', labelrotation=45)
plt.ylabel("Ad time (seconds)", fontsize=14)

# Set the x-axis tick labels to country codes
plt.xticks(range(len(country_codes)), country_codes)
plt.show()

"""### Figure 4:

Ad themes in labelled content.
"""

melted_agg1["Region"].replace({"High":"High Policy", "Low":"Low Policy"}, inplace=True)

melted_agg1

plt.figure(figsize=(6, 2))
fig = sns.barplot(x="category", y="proportion", hue="Region", data=melted_agg1.query('category in ["inappropriate","child-directed","irrelevant"]'),palette=[GREEN,RED])

plt.xlabel("Category of Ads", fontsize=14)
plt.ylabel("Proportion of Ads", fontsize=14)
plt.legend(fontsize=12)
plt.tick_params(axis='x', labelsize=13)
plt.tick_params(axis='y', labelsize=13)

fig.set_axisbelow(True)
plt.grid(axis='y',color='gray', linestyle='dashed')

melted_agg1

print("Percentage of ads that are Inappropriate in High Policy Regions: ",round(melted_agg1[(melted_agg1['Region']=='High Policy')&(melted_agg1['category']=='inappropriate')]['proportion'].values[0]*100,1),'%')
print("Percentage of ads that are Child Directed in High Policy Regions: ",round(melted_agg1[(melted_agg1['Region']=='High Policy')&(melted_agg1['category']=='child-directed')]['proportion'].values[0]*100,1),'%')
print("Percentage of ads that are Irrelevant in High Policy Regions: ",round(melted_agg1[(melted_agg1['Region']=='High Policy')&(melted_agg1['category']=='irrelevant')]['proportion'].values[0]*100,1),'%')
print("Percentage of ads that are Inappropriate in Low Policy Regions: ",round(melted_agg1[(melted_agg1['Region']=='Low Policy')&(melted_agg1['category']=='inappropriate')]['proportion'].values[0]*100,1),'%')
print("Percentage of ads that are Child Directed in Low Policy Regions: ",round(melted_agg1[(melted_agg1['Region']=='Low Policy')&(melted_agg1['category']=='child-directed')]['proportion'].values[0]*100,1),'%')
print("Percentage of ads that are Irrelevant in Low Policy Regions: ",round(melted_agg1[(melted_agg1['Region']=='Low Policy')&(melted_agg1['category']=='irrelevant')]['proportion'].values[0]*100,1),'%')

print("Percentage of inappropriate ads that are Mild in High Policy Regions: ",round(melted_inapp_rating_df[(melted_inapp_rating_df['Region']=='High')&(melted_inapp_rating_df['category']=='Mild')]['value'].values[0]*100,1),'%')
print("Percentage of inappropriate ads that are Moderate Directed in High Policy Regions: ",round(melted_inapp_rating_df[(melted_inapp_rating_df['Region']=='High')&(melted_inapp_rating_df['category']=='Moderate')]['value'].values[0]*100,1),'%')
print("Percentage of inappropriate ads that are Extreme in High Policy Regions: ",round(melted_inapp_rating_df[(melted_inapp_rating_df['Region']=='High')&(melted_inapp_rating_df['category']=='Extreme')]['value'].values[0]*100,1),'%')
print("Percentage of inappropriate ads that are Mild in Low Policy Regions: ",round(melted_inapp_rating_df[(melted_inapp_rating_df['Region']=='Low')&(melted_inapp_rating_df['category']=='Mild')]['value'].values[0]*100,1),'%')
print("Percentage of inappropriate ads that are Moderate Directed in Low Policy Regions: ",round(melted_inapp_rating_df[(melted_inapp_rating_df['Region']=='Low')&(melted_inapp_rating_df['category']=='Moderate')]['value'].values[0]*100,1),'%')
print("Percentage of inappropriate ads that are Extreme in Low Policy Regions: ",round(melted_inapp_rating_df[(melted_inapp_rating_df['Region']=='Low')&(melted_inapp_rating_df['category']=='Extreme')]['value'].values[0]*100,1),'%')

"""### Figure 5:

Distribution of ad time on unlabelled and labelled videos across both low and high policy environments.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Patch

fig = plt.figure(figsize=(7, 3))
sns.set_style("white")
plt.grid(axis='y', linestyle='dashed')

box_order = ["L.P Labelled", "L.P Unlabelled", "H.P Labelled", "H.P Unlabelled"]

sns.boxplot(x="segment", y="ad time", data=f_cdf[f_cdf["ad time"] < 1500], whis=2.5,
            palette=[sns.color_palette("Set3")[3], sns.color_palette("Set3")[3],
                     sns.color_palette("Set3")[0], sns.color_palette("Set3")[0]],
            order=box_order)

plt.yscale('log')  # Set y-axis to log scale

# plt.title("Distribution of total ad watchtime per video")
plt.xlabel("Country")
plt.ylabel("Total Ad Watch Time (seconds)")

labels = ['High Policy', 'Low Policy']
colors = [sns.color_palette("Set3")[0], sns.color_palette("Set3")[3]]
legend_patches = [Patch(color=color, label=label) for color, label in zip(colors, labels)]
plt.legend(handles=legend_patches, loc="upper right")

plt.tick_params(axis='x', labelsize=12)
plt.tick_params(axis='y', labelsize=13)
plt.xlabel("Region", fontsize=14)
plt.ylabel("log(Ad time)", fontsize=14)

plt.show()

tempmeans = f_cdf.groupby("segment")["ad time"].mean()
for segment, mean in tempmeans.items():
    print(f"Mean ad time for {segment}: {mean:.0f} seconds")

print("Median ad time for Labelled Low Policy regions: ",round(f_cdf[f_cdf['segment']=='L.P Labelled']['ad time'].median(),0),'secs')
print("Median ad time for Unlabelled Low Policy regions: ",round(f_cdf[f_cdf['segment']=='L.P Unlabelled']['ad time'].median(),0),'secs')
print("Median ad time for Labelled High Policy regions: ",round(f_cdf[f_cdf['segment']=='H.P Labelled']['ad time'].median(),0),'secs')
print("Median ad time for Unlabelled High Policy regions: ",round(f_cdf[f_cdf['segment']=='H.P Unlabelled']['ad time'].median(),0),'secs')

"""### Appendix Figures

Mean ad frequency across labelled and unlabelled content in (a) high policy regions and (b) low policy regions. Error bars show 95% confidence intervals.
"""

values_high_pol_labelled,values_high_pol_unlabelled,five_plus_high_pol_labelled,five_plus_high_pol_unlabelled=histobar_maker(high_pol_dist,uhigh_pol_dist)
values_low_pol_labelled,values_low_pol_unlabelled,five_plus_low_pol_labelled,five_plus_low_pol_unlabelled=histobar_maker(low_pol_dist,ulow_pol_dist)
uvalues_low_pol,uvalues_high_pol,ufive_plus_low_pol,ufive_plus_high_pol=histobar_maker(ulow_pol_dist,uhigh_pol_dist)

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

fig, ax2 = plt.subplots(figsize=(5, 4))
x = np.arange(len(values_high_pol_labelled))  # Generate x-axis values
sns.set_style("white")
ax1.grid(axis='y', linestyle='dashed')

bar_width = 0.35  # Width of each bar
means = pd.DataFrame(columns=["mean", "ci", "region"])

mean = high_pol_dist[high_pol_dist["region"] == "high policy"]["num of ads"].mean()
ci = high_pol_dist[high_pol_dist["region"] == "high policy"]["num of ads"].sem() * 1.96
region = "H.P Labelled"
means = means.append({"mean": mean, "ci": ci, "region": region}, ignore_index=True)

mean = uhigh_pol_dist[uhigh_pol_dist["region"] == "high policy"]["num of ads"].mean()
ci = uhigh_pol_dist[uhigh_pol_dist["region"] == "high policy"]["num of ads"].sem() * 1.96
region = "H.P Unlabelled"
means = means.append({"mean": mean, "ci": ci, "region": region}, ignore_index=True)

# Define colors for the bars
colors = [sns.color_palette("Set3")[3], sns.color_palette("Set3")[0]]

# Plot the bar plot with means and confidence intervals
ax2.bar(means['region'], means['mean'], color=colors, capsize=8, alpha=0.8,width=0.3)
ax2.set_xlim(-0.7, len(means) - 1 + 0.7)

error_config = {'color': 'grey', 'linewidth': 0.8, 'capsize': 7}
ax2.errorbar(means['region'], means['mean'], yerr=means['ci'], fmt='none', ecolor='black', **error_config)
ax = plt.gca()

# Increase the size of x-axis and y-axis tick labels
ax.tick_params(axis='x', labelsize=13)
ax.tick_params(axis='y', labelsize=13)

# Add labels and title
ax2.set_xlabel('Region',fontsize=14)
ax2.set_ylabel('Mean Number of Ads',fontsize=14)
# plt.title('Mean Number of Ads with 95% Confidence Intervals')
ax2.set_ylim(0, 2.05)

# plt.xticks(x, x)  # Set x-axis ticks to the indices
plt.tight_layout()  # Adjust the layout

# Show the plot
print("Figure 5(a)")
plt.show()

high_labelled_VS_high_unlabelled_mean_df = means

print("Average ad count per video for High Policy Labelled videos",round(high_labelled_VS_high_unlabelled_mean_df[high_labelled_VS_high_unlabelled_mean_df['region']=='H.P Labelled']['mean'].values[0],1))
print("Average ad count per video for High Policy UnLabelled videos",round(high_labelled_VS_high_unlabelled_mean_df[high_labelled_VS_high_unlabelled_mean_df['region']=='H.P Unlabelled']['mean'].values[0],1))

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

fig, ax2 = plt.subplots(figsize=(5, 4))
x = np.arange(len(values_low_pol_labelled))  # Generate x-axis values
sns.set_style("white")
ax1.grid(axis='y', linestyle='dashed')
bar_width = 0.35  # Width of each bar
means = pd.DataFrame(columns=["mean", "ci", "region"])

mean = low_pol_dist[low_pol_dist["region"] == "low policy"]["num of ads"].mean()
ci = low_pol_dist[low_pol_dist["region"] == "low policy"]["num of ads"].sem() * 1.96
region = "L.P Labelled"
means = means.append({"mean": mean, "ci": ci, "region": region}, ignore_index=True)

mean = ulow_pol_dist[ulow_pol_dist["region"] == "low policy"]["num of ads"].mean()
ci = ulow_pol_dist[ulow_pol_dist["region"] == "low policy"]["num of ads"].sem() * 1.96
region = "L.P Unlabelled"
means = means.append({"mean": mean, "ci": ci, "region": region}, ignore_index=True)

# Define colors for the bars
colors = [sns.color_palette("Set3")[3], sns.color_palette("Set3")[0]]

# Plot the bar plot with means and confidence intervals
ax2.bar(means['region'], means['mean'], color=colors, capsize=8, alpha=0.8, width=0.3)
ax2.set_xlim(-0.7, len(means) - 1 + 0.7)
ax2.set_ylim(0, 2.05)

error_config = {'color': 'grey', 'linewidth': 0.8, 'capsize': 7}
ax2.errorbar(means['region'], means['mean'], yerr=means['ci'], fmt='none', ecolor='black', **error_config)
ax = plt.gca()

# Increase the size of x-axis and y-axis tick labels
ax.tick_params(axis='x', labelsize=13)
ax.tick_params(axis='y', labelsize=13)

# Add labels and title
ax2.set_xlabel('Region', fontsize=14)
ax2.set_ylabel('Mean Number of Ads', fontsize=14)

plt.tight_layout()  # Adjust the layout

# Show the plot
print("Figure 5(b)")
plt.show()
low_labelled_VS_low_unlabelled_mean_df = means

print("Average ad count per video for Low Policy Labelled videos",round(low_labelled_VS_low_unlabelled_mean_df[low_labelled_VS_low_unlabelled_mean_df['region']=='L.P Labelled']['mean'].values[0],1))
print("Average ad count per video for Low Policy UnLabelled videos",round(low_labelled_VS_low_unlabelled_mean_df[low_labelled_VS_low_unlabelled_mean_df['region']=='L.P Unlabelled']['mean'].values[0],1))

statistic, p_value = stats.ttest_ind(ulow_pol_dist[ulow_pol_dist["region"] == "low policy"]["num of ads"].dropna(),low_pol_dist[low_pol_dist["region"] == "low policy"]["num of ads"].dropna())
print("Welch t-test p-value comparing ad frequency per video across labelled and unlabelled videos in low policy regions:", p_value)

"""### Numerical Figures"""

print(f_cdf[(f_cdf["segment"] == "L.P Labelled")|(f_cdf["segment"] == "L.P Unlabelled")]['ad time'].mean())
f_cdf.head()

print("Average ad watch time in high policy regions: ")
print("Average ad watch time in low policy regions: ")

####stats for histogram graphs:
print("--------------------------------")
print("STATS FOR LABELLED HIGH POLICY VS LABELLED LOW POLICY:")
print()
print("Stats for the Comparison of distribution of ad % frequency per video: Low Policy Region vs High Policy Region") #((high_pol-low_pol)/low_pol)*100
print("Absolute difference between 0 across datasets: ",(values_high_pol[0]-values_low_pol[0]),'%     high_pol-low_pol')
print("Absolute difference between 5 and 5+ across datasets: ",(five_plus_high_pol-five_plus_low_pol),'%    high_pol-low_pol')

print("Percentage of videos with more than 4 ads: ")
print("high: ",five_plus_high_pol,"low: ",five_plus_low_pol)
print()
print("--------------------------------")

print()
print()

print("--------------------------------")
print("STATS FOR UNLABELLED HIGH POLICY VS LABELLED LOW POLICY:")
print()
print("Stats for the Comparison of distribution of ad % frequency per video: Low Policy Region vs High Policy Region") #((high_pol-low_pol)/low_pol)*100
print("Absolute difference between 0 across datasets: ",(uvalues_high_pol[0]-uvalues_low_pol[0]),'%     high_pol-low_pol')
print("Absolute difference between 5 and 5+ across datasets: ",(ufive_plus_high_pol-ufive_plus_low_pol),'%    high_pol-low_pol')

print("Percentage of videos with more than 4 ads: ")
print("high: ",ufive_plus_high_pol,"low: ",ufive_plus_low_pol)
print()
print("--------------------------------")

print()
print()

print("--------------------------------")
print("STATS FOR UNLABELLED HIGH POLICY VS LABELLED HIGH POLICY:")
print()
print("Stats for the Comparison of distribution of ad % frequency per video: High policy unlabelled VS High policy labelled")
print("Absolute difference between 0 across datasets: ",(values_high_pol_unlabelled[0]-values_high_pol_labelled[0]),'%     values_high_pol_unlabelled-values_high_pol_labelled')
print("Absolute difference between 5 and 5+ across datasets: ",(five_plus_high_pol_unlabelled-five_plus_high_pol_labelled),'%    five_plus_high_pol_unlabelled-five_plus_high_pol_labelled')

print("Percentage of videos with more than 4 ads: ")
print("Unlabelled: ",five_plus_high_pol_unlabelled,"Labelled: ",five_plus_high_pol_labelled)
print()
print("--------------------------------")

print()
print()

print("--------------------------------")
print("STATS FOR UNLABELLED LOW POLICY VS LABELLED LOW POLICY:")
print()
print("Stats for the Comparison of distribution of ad % frequency per video: Low policy unlabelled VS Low policy labelled")
print("Absolute difference between 0 across datasets: ",(values_low_pol_unlabelled[0]-values_low_pol_labelled[0]),'%     values_low_pol_unlabelled-values_low_pol_labelled')
print("Absolute difference between 5 and 5+ across datasets: ",(five_plus_low_pol_unlabelled-five_plus_low_pol_labelled),'%    five_plus_low_pol_unlabelled-five_plus_low_pol_labelled')

print("Percentage of videos with more than 4 ads: ")
print("Unlabelled: ",five_plus_low_pol_unlabelled,"Labelled: ",five_plus_low_pol_labelled)
print()
print("--------------------------------")

low_labelled_VS_low_unlabelled_mean_df

print("Means of ad frequency")
print("------------------------------------------------")
print("High Policy Labelled Mean: ",high_labelled_VS_high_unlabelled_mean_df['mean'][0])
print("High Policy UnLabelled Mean: ",high_labelled_VS_high_unlabelled_mean_df['mean'][1])
print("Low Policy Labelled Mean: ",low_labelled_VS_low_unlabelled_mean_df['mean'][0])
print("Low Policy UnLabelled Mean: ",low_labelled_VS_low_unlabelled_mean_df['mean'][1])

mean_num_ads_france = reg_dist[reg_dist['country'] == 'France']['num of ads'].mean()
mean_num_ads_pakistan = nreg_dist[nreg_dist['country'] == 'Pakistan']['num of ads'].mean()
print("Mean number of ads per video in France: ",mean_num_ads_france)
print("Mean number of ads per video in Pakistan: ",mean_num_ads_pakistan)

#to be included
countrywise_ad_count_stats = prepare_ad_count_df_u_ul()
print("Total ad count for Labelled High Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Labelled') & (countrywise_ad_count_stats['region'] == 'high')]['total ad count'].sum())
print("Unique ad count for Labelled High Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Labelled') & (countrywise_ad_count_stats['region'] == 'high')]['unique ad count'].sum())
print("Total ad count for UnLabelled High Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Unlabelled') & (countrywise_ad_count_stats['region'] == 'high')]['total ad count'].sum())
print("Unique ad count for UnLabelled High Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Unlabelled') & (countrywise_ad_count_stats['region'] == 'high')]['unique ad count'].sum())
print()

print("Total ad count for Labelled Low Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Labelled') & (countrywise_ad_count_stats['region'] == 'low')]['total ad count'].sum())
print("Unique ad count for Labelled Low Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Labelled') & (countrywise_ad_count_stats['region'] == 'low')]['unique ad count'].sum())
print("Total ad count for UnLabelled Low Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Unlabelled') & (countrywise_ad_count_stats['region'] == 'low')]['total ad count'].sum())
print("Unique ad count for UnLabelled Low Policy: ",countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Unlabelled') & (countrywise_ad_count_stats['region'] == 'low')]['unique ad count'].sum())
print()

#to be included
uniqueVidHighPol = countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Labelled') & (countrywise_ad_count_stats['region'] == 'high')]['unique ad count'].sum()
uniqueVidLowPol = countrywise_ad_count_stats[(countrywise_ad_count_stats['type'] == 'Labelled') & (countrywise_ad_count_stats['region'] == 'low')]['unique ad count'].sum()
percentageDiffUniqueAds = (uniqueVidHighPol-uniqueVidLowPol)/uniqueVidLowPol
print(f"High policy regions have {round(percentageDiffUniqueAds*100,1)}% more unique ads in comparison to Low policy regions")

"""### CDFs for ad time"""

fig = plt.figure(figsize=(12, 5))
sns.set_style("white")
plt.grid(axis='y', linestyle='dashed')
ax = sns.ecdfplot(data=pol[pol["duration"] < 1500],x="duration", marker='',hue="pol",ls='solid',  palette=[sns.color_palette("Set3")[3],sns.color_palette("Set3")[0]],linewidth=2)
# Extract the legend information from the seaborn plot
legend = ax.get_legend()

# Get the legend handles and labels
legend_handles, legend_labels = legend.legendHandles, [text.get_text() for text in legend.get_texts()]

plt.axvline(360, color="red", linestyle='-', label="YouTube Ad Limit")
# plt.axvline(mean_high, color=sns.color_palette("Set2")[1], linestyle='--', label="High Policy Mean")
# plt.axvline(mean_low, color=sns.color_palette("Set2")[0], linestyle='--', label="Low Policy Mean")
plt.axvline(60, color="maroon", linestyle='-', label="YT Kids Ad Limit")
plt_handles, plt_labels = plt.gca().get_legend_handles_labels()
new_labels = ['Low Policy Regions', 'High Policy Regions']


combined_handles = legend_handles + plt_handles
combined_labels = new_labels + plt_labels
plt.legend(combined_handles, combined_labels,fontsize=18)

lines = plt.gca().lines

# Specify the x-coordinate of the vertical line
vertical_line_x = 60

# Initialize variables to store the y-coordinates of the intersection points
y_intersect_A = None
y_intersect_B = None

# Iterate through the lines to find the curves of interest
for line in lines:

    if line.get_label() == '_child0':
        # Interpolate the y-value of curve A at the specified x-coordinate
        y_intersect_A = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())
    elif line.get_label() == '_child1':
        # Interpolate the y-value of curve B at the specified x-coordinate
        y_intersect_B = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())


# # Plot the intersection points
plt.plot(vertical_line_x, y_intersect_A, 'o', color="maroon", label='Intersection A')
plt.plot(vertical_line_x, y_intersect_B, 'o', color="maroon", label='Intersection B')
print(y_intersect_A, y_intersect_B)
plt.text(60+10, y_intersect_A+0.02, f"{round(y_intersect_A*100,2)}%", ha='left', va='center_baseline', color='gray', fontsize=16)
plt.text(60+10, y_intersect_B+0.02, f"{round(y_intersect_B*100,2)}%", ha='left', va='center_baseline', color='gray', fontsize=16)


plt.axhline(y_intersect_A, color="gray", linestyle=':', label="YouTube Ad Limit")
plt.axhline(y_intersect_B, color="gray", linestyle=':', label="YouTube Ad Limit")


plt.tick_params(axis='x',labelsize=20)
plt.tick_params(axis='y',labelsize=20)
plt.xlabel("Ad Duration",fontsize=24)
plt.ylabel("Proportion",fontsize=24)



# plt.grid(True, color="lightgray", linestyle="-", linewidth=0.5)
# ax.set_axisbelow(True)
# plt.grid(axis='y', linestyle='dashed',color='gray')

# ax.grid(color='gray', linestyle='dashed')
plt.show()

fig = plt.figure(figsize=(12, 5))
sns.set_style("white")
plt.grid(axis='y', linestyle='dashed')
ax = sns.ecdfplot(data=upol[upol["duration"] < 1500],x="duration", marker='',hue="pol",ls='solid',  palette=[sns.color_palette("Set3")[3],sns.color_palette("Set3")[0]],linewidth=2)
# Extract the legend information from the seaborn plot
legend = ax.get_legend()

# Get the legend handles and labels
legend_handles, legend_labels = legend.legendHandles, [text.get_text() for text in legend.get_texts()]

plt.axvline(15, color="red", linestyle='-', label="YouTube Ad Limit")
# plt.axvline(mean_high, color=sns.color_palette("Set2")[1], linestyle='--', label="High Policy Mean")
# plt.axvline(mean_low, color=sns.color_palette("Set2")[0], linestyle='--', label="Low Policy Mean")
plt_handles, plt_labels = plt.gca().get_legend_handles_labels()
new_labels = ['Low Policy Regions', 'High Policy Regions']


combined_handles = legend_handles + plt_handles
combined_labels = new_labels + plt_labels
plt.legend(combined_handles, combined_labels,fontsize=18)

lines = plt.gca().lines

# Specify the x-coordinate of the vertical line
vertical_line_x = 16

# Initialize variables to store the y-coordinates of the intersection points
y_intersect_A = None
y_intersect_B = None

# Iterate through the lines to find the curves of interest
for line in lines:

    if line.get_label() == '_child0':
        # Interpolate the y-value of curve A at the specified x-coordinate
        y_intersect_A = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())
    elif line.get_label() == '_child1':
        # Interpolate the y-value of curve B at the specified x-coordinate
        y_intersect_B = np.interp(vertical_line_x, line.get_xdata(), line.get_ydata())


# # Plot the intersection points
plt.plot(vertical_line_x, y_intersect_A, 'o', color="maroon", label='Intersection A')
plt.plot(vertical_line_x, y_intersect_B, 'o', color="maroon", label='Intersection B')
print(y_intersect_A, y_intersect_B)
plt.text(60+110, y_intersect_A+0.02, f"{round(y_intersect_A*100,2)}%", ha='left', va='center_baseline', color='gray', fontsize=16)
plt.text(60+110, y_intersect_B+0.02, f"{round(y_intersect_B*100,2)}%", ha='left', va='center_baseline', color='gray', fontsize=16)


plt.axhline(y_intersect_A, color="gray", linestyle=':', label="YouTube Ad Limit")
plt.axhline(y_intersect_B, color="gray", linestyle=':', label="YouTube Ad Limit")


plt.tick_params(axis='x',labelsize=20)
plt.tick_params(axis='y',labelsize=20)
plt.xlabel("Ad Duration",fontsize=24)
plt.ylabel("Proportion",fontsize=24)

uk_times['country'] = ['UK' for i in range(len(uk_times['ad time']))]
us_times['country'] = ['US' for i in range(len(us_times['ad time']))]
fr_times['country'] = ['France' for i in range(len(fr_times['ad time']))]
sri_times['country'] = ['Sri Lanka' for i in range(len(sri_times['ad time']))]
ban_times['country'] = ['Bangladesh' for i in range(len(ban_times['ad time']))]
pak_times['country'] = ['Pakistan' for i in range(len(pak_times['ad time']))]
ger_times['country'] = ['Germany' for i in range(len(ger_times['ad time']))]
swed_times['country'] = ['Sweden' for i in range(len(swed_times['ad time']))]
mor_times['country'] = ['Morocco' for i in range(len(mor_times['ad time']))]
venez_times['country'] = ['Venezuela' for i in range(len(venez_times['ad time']))]
us_ul_times['country'] = ['US UL' for i in range(len(us_ul_times['ad time']))]
pak_ul_times['country'] = ['Pakistan UL' for i in range(len(pak_ul_times['ad time']))]


# ul_times['country'] = ['Unlabelled' for i in range(len(ul_times['ad time']))]

uk_times['segment'] = ['High' for i in range(len(uk_times['ad time']))]
us_times['segment'] = ['High' for i in range(len(us_times['ad time']))]
fr_times['segment'] = ['High' for i in range(len(fr_times['ad time']))]
sri_times['segment'] = ['Low' for i in range(len(sri_times['ad time']))]
ban_times['segment'] = ['Low' for i in range(len(ban_times['ad time']))]
mor_times['segment'] = ['Low' for i in range(len(mor_times['ad time']))]
venez_times['segment'] = ['Low' for i in range(len(venez_times['ad time']))]
ger_times['segment'] = ['High' for i in range(len(ger_times['ad time']))]
swed_times['segment'] = ['High' for i in range(len(swed_times['ad time']))]
pak_times['segment'] = ['Low' for i in range(len(pak_times['ad time']))]
us_ul_times['segment'] = ['High UL' for i in range(len(us_ul_times['ad time']))]
pak_ul_times['segment'] = ['Low UL' for i in range(len(pak_ul_times['ad time']))]

# ul_times['segment'] = ['Low' for i in range(len(ul_times['ad time']))]


reg_regional_cdf=pd.concat([uk_times, us_times, fr_times, ger_times, swed_times], axis=0)
nreg_regional_cdf=pd.concat([sri_times, ban_times, pak_times,mor_times, venez_times], axis=0)
ul_regional_cdf=pd.concat([pak_times, pak_ul_times, us_times, us_ul_times], axis=0)

reg_regional_cdf.reset_index(level=0, inplace=True)
nreg_regional_cdf.reset_index(level=0, inplace=True)
# reg_regional_cdf['segment'] = ['High' for i in range(len(reg_regional_cdf['ad time']))]
# nreg_regional_cdf['segment'] = ['Low' for i in range(len(nreg_regional_cdf['ad time']))]

comb_cdf = pd.concat([reg_regional_cdf,nreg_regional_cdf],  axis=0)
comb_cdf.reset_index(level=0, inplace=True)

ul_regional_cdf.reset_index(level=0, inplace=True)

test = pd.concat([reg_regional_cdf,nreg_regional_cdf])

pak_mean = pak_times.mean()
us_mean = us_times.mean()
uk_mean = uk_times.mean()
fr_mean = fr_times.mean()
ger_mean = ger_times.mean()
swed_mean = swed_times.mean()
ban_mean = ban_times.mean()
sri_mean = sri_times.mean()
mor_mean = mor_times.mean()
venez_mean = venez_times.mean()

pak_mean

venez_mean

ul_regional_cdf["ratio"] = ul_regional_cdf["ad time"]/ul_regional_cdf["video time"]

"""### Boxplots - distribution of ad time"""

test.head()

# test = test.reset_index()
test.drop("index", axis=1)
test

def proportion(df):
  return len(df[(df["total ad count"]>0)])

def skippableN(df):
  return len(df[(df["skip ad count"]>0)])

def unskippableN(df):
  return len(df[(df["skip ad count"]>0)])

props = {"skippable":[], "unskippable": []}
props["skippable"].append(skippableN(pak_df))
props["skippable"].append(skippableN(sri_df))
props["skippable"].append(skippableN(ban_df))

rprops = {"skippable":[], "unskippable": []}
rprops["skippable"].append(skippableN(us_df))
rprops["skippable"].append(skippableN(uk_df))
rprops["skippable"].append(skippableN(fr_df))

def getDist(df, country):
  ad_num_dist = pd.DataFrame()
  ad_num_dist["num of ads"] = df["total ad count"].copy()
  print(len(ad_num_dist['num of ads']))
  ad_num_dist["ad type"] = ['video' for i in range(len(ad_num_dist['num of ads']))]
  ad_num_dist["country"] = [country for i in range(len(ad_num_dist["num of ads"]))]
  return ad_num_dist

"""### Distribution of ad frequency"""

nreg_dist

pak_times

nreg_props = []
nreg_props.append(proportion(sri_df))
nreg_props.append(proportion(pak_df))
nreg_props.append(proportion(ban_df))

pak_df.head()

category_dict = {1: "Film & Animation", 2: "Autos & Vehicles", 10: "Music", 15: "Pets & Animals",17:"Sports",18:"Short Movies", 19:"Travel & Events", 20:"Gaming",21:"Videoblogging",22:"People & Blogs",
23:"Comedy",
24:"Entertainment",
25:"News & Politics",
26:"Howto & Style",
27:"Education",
28:"Science & Technology",
29:"Nonprofits & Activism",
30:"Movies",
31:"Anime/Animation",
32:"Action/Adventure",
33:"Classics",
34:"Comedy",
35:"Documentary",
36:"Drama",
37:"Family",
38:"Foreign",
39:"Horror",
40:"Sci-Fi/Fantasy",
41:"Thriller",
42:"Shorts",
43:"Shows",
44:"Trailers"}

def categoryAllocator(ad_df):
  d_names = category_dict.values()
  d_counts = {}

  for d in ad_df['categoryId']:

    if (d == "N/I"):
      continue

    d = int(d)
    if d_counts.get(d):
      d_counts[d]=d_counts[d]+1
    else:
      d_counts[d]=1

  d_final = {category_dict[k]:d_counts[k] for k in d_counts.keys()}
  return d_final

pak_cat = pd.DataFrame.from_dict(categoryAllocator(pak_ad_df).items())
pak_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
pak_cat = pak_cat.sort_values("Count", ascending=False)


sri_cat = pd.DataFrame.from_dict(categoryAllocator(sri_ad_df).items())
sri_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
sri_cat = sri_cat.sort_values("Count", ascending=False)


ban_cat = pd.DataFrame.from_dict(categoryAllocator(ban_ad_df).items())
ban_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
ban_cat = ban_cat.sort_values("Count", ascending=False)

uk_cat = pd.DataFrame.from_dict(categoryAllocator(uk_ad_df).items())
uk_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
uk_cat = uk_cat.sort_values("Count", ascending=False)

us_cat = pd.DataFrame.from_dict(categoryAllocator(us_ad_df).items())
us_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
us_cat = us_cat.sort_values("Count", ascending=False)


fr_cat = pd.DataFrame.from_dict(categoryAllocator(fr_ad_df).items())
fr_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
fr_cat = fr_cat.sort_values("Count", ascending=False)



# ul_cat = pd.DataFrame.from_dict(categoryAllocator(ul_ad_df).items())
# ul_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
# ul_cat = ul_cat.sort_values("Count", ascending=False)

regCat = pd.concat([us_ad_df,uk_ad_df,fr_ad_df])
nregCat = pd.concat([pak_ad_df,sri_ad_df,ban_ad_df])

reg_cat = pd.DataFrame.from_dict(categoryAllocator(regCat).items())
reg_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
reg_cat = reg_cat.sort_values("Count", ascending=False)

nreg_cat = pd.DataFrame.from_dict(categoryAllocator(nregCat).items())
nreg_cat.rename(columns={0: 'Category', 1: 'Count'}, inplace= True)
nreg_cat = nreg_cat.sort_values("Count", ascending=False)

reg_cat['Percentage']=(reg_cat['Count']/reg_cat['Count'].sum())*100
reg_cat

nreg_cat['Percentage']=(nreg_cat['Count']/nreg_cat['Count'].sum())*100
nreg_cat

pak_cat['Percentage']=(pak_cat['Count']/pak_cat['Count'].sum())*100

pak_cat

"""# Qualitative Analysis"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib

GREEN = sns.color_palette("Set3")[0]
RED = sns.color_palette("Set3")[3]

rgb_colors = sns.color_palette("Set3")
hex_colors = list(map(matplotlib.colors.rgb2hex, rgb_colors))

df = pd.read_csv('stats1.csv')
df

## new
df.rename(columns = {'inaudible':'ambiguous'}, inplace = True)
labelled_df = df[df['Content']=="Labelled"].copy()
labelled_df

high1_df =  df[(df['Content']=="Labelled") & (df['Region']=="High")].copy()
high1_df.drop(columns=['Region','Content'],inplace=True)
high1_df

melted_df['proportion'] = melted_df['value']/150

display(melted_df)

"""## Low policy analysis

"""

low1_df =  df[(df['Content']=="Labelled") & (df['Region']=="Low")].copy()
low1_df.drop(columns=['Region','Content'],inplace=True)
low1_df

low_melted_df['proportion'] = low_melted_df['value']/150

display(low_melted_df)

"""## All countries (Labelled)

## Aggregated (Labelled)
"""

agg_df = pd.read_csv('aggPrimaryTags.csv')
agg_df

new_agg_df = agg_df.copy()

new_agg_df["child-directed"] = new_agg_df["child-directed"]/750
new_agg_df["irrelevant"] = new_agg_df["irrelevant"]/750
new_agg_df["inappropriate"] = new_agg_df["inappropriate"]/750
new_agg_df["excluded"] = new_agg_df["excluded"]/750
new_agg_df["ambiguous"] = new_agg_df["ambiguous"]/750

new_agg_df[new_agg_df['Region']=="Low"]

new_agg_df[new_agg_df['Region']=="High"]

labelled_agg = agg_df[agg_df['Content']=="Labelled"].copy()
labelled_agg.drop(columns='Content',inplace=True)
labelled_agg

# Melt the DataFrame to convert columns to rows
melted_agg1 = pd.melt(labelled_agg, id_vars=["Region"], var_name="category", value_name="value")
melted_agg1

melted_agg1['proportion'] = melted_agg1['value']/750

melted_agg1

melted_agg1.query('category in ["inappropriate","child-directed"]')

"""# Google Video Intelligence analysis"""

import glob
import os
import json
import pandas as pd

folder_path = '/content/drive/My Drive/Topics in IR Project/API json files/*'
file_pattern = '*.json'

data = []

json_files = glob.glob(os.path.join(folder_path, file_pattern))

for file_path in json_files:
    parent_folder_name = os.path.basename(os.path.dirname(file_path))
    with open(file_path, 'r') as f:
        json_data = json.load(f)
    try:
      explicit_frames = json_data['annotation_results'][0]['explicit_annotation']['frames']
    except:
      explicit_frames = json_data['annotation_results'][1]['explicit_annotation']['frames']

    file_name = os.path.basename(file_path)
    frame_times = []
    explicit_likelihoods = []


    for frame in explicit_frames:
        time_offset = frame['time_offset']
        frame_time = time_offset.get('seconds', 0) + time_offset.get('nanos', 0) / 1e9

        pornography_likelihood = frame['pornography_likelihood']

        frame_times.append(frame_time)
        explicit_likelihoods.append(pornography_likelihood)

    data.append({
        'Parent Folder': parent_folder_name,
        'File Name': file_name,
        'Frame Times': frame_times,
        'Pornography Likelihoods': explicit_likelihoods
    })

df = pd.DataFrame(data)

df.head()

import ast

stats = {}
for index, row in df.iterrows():
    country = row['Parent Folder']
    video = row['File Name']
    porn_likelihoods = row['Pornography Likelihoods']
    tally = -1
    if "VERY_LIKELY" in porn_likelihoods:
      tally = 0
    elif "LIKELY" in porn_likelihoods:
      tally = 1
    elif "POSSIBLE" in porn_likelihoods:
      tally = 2
    if country not in stats:
      stats[country] = {
      'LIKELY': 0,
      'VERY_LIKELY': 0,
      'POSSIBLE': 0,
      'Any': 0,
      'Total': 0
      }
    if tally > -1:
      if tally == 0:
        # stats[country]['LIKELY'] += 1
        stats[country]['VERY_LIKELY'] += 1
        # stats[country]['POSSIBLE'] += 1
      elif tally == 1:
        stats[country]['LIKELY'] += 1
        # stats[country]['POSSIBLE'] += 1
      elif tally == 2:
        stats[country]['POSSIBLE'] += 1
      stats[country]['Any'] += 1
    stats[country]['Total'] += 1

final_df = pd.DataFrame(stats).transpose().reset_index()
final_df.rename(columns={'index': 'Country'}, inplace=True)

print(final_df)

print("Possible ", final_df["Any"].sum())
print("Likely ", final_df["LIKELY"].sum())
print("V likely ", final_df["VERY_LIKELY"].sum())
print("Total", final_df["Total"].sum())

print("Possible %", final_df["Any"].sum()/final_df["Total"].sum()*100)
print("Likely %", final_df["LIKELY"].sum()/final_df["Total"].sum()*100)
print("V likely %", final_df["VERY_LIKELY"].sum()/final_df["Total"].sum()*100)

final_df

"""# Pre roll ads (on labelled videos)"""

import pandas as pd

pak_df = pd.read_csv("Pakistan.csv")
sri_df = pd.read_csv("Sri Lanka.csv")
mor_df = pd.read_csv("Morocco.csv")
venez_df = pd.read_csv("Venezuela.csv")
ban_df = pd.read_csv("Bangladesh.csv")
us_df = pd.read_csv("US.csv")
uk_df = pd.read_csv("UK.csv")
fr_df = pd.read_csv("France.csv")
ger_df = pd.read_csv("Germany.csv")
swed_df = pd.read_csv("Sweden.csv")

def preRoll(df):
  tot = 0
  count = 0
  for k in df["video ad ids"]:
    ads = ast.literal_eval(k)
    if (len(ads) < 1):
      continue
    if ("Hxy0AEktdaQ " in ads) or "wMKvGAHJ-t4 " in ads:
      count +=1
    tot +=1

  return count/tot*100

import ast
prerollStats = {}
prerollStats["UK"] = preRoll(uk_df)
prerollStats["US"] = preRoll(us_df)
prerollStats["Germany"] = preRoll(ger_df)
prerollStats["Sweden"] = preRoll(swed_df)
prerollStats["France"] = preRoll(fr_df)
prerollStats["Sri Lanka"] = preRoll(sri_df)
prerollStats["Bangladesh"] = preRoll(ban_df)
prerollStats["Pakistan"] = preRoll(pak_df)
prerollStats["Venezuela"] = preRoll(venez_df)
prerollStats["Morocco"] = preRoll(mor_df)

preRollres = pd.DataFrame(prerollStats.items())

print("Percentage of videos which had ads that also had a pre roll:")
preRollres.mean()

prerollStats